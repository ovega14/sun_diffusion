{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de865903-d5d6-4388-bfc9-08af15b67b51",
   "metadata": {},
   "source": [
    "# Denoising using a trained Score Network for the ${\\rm SU}(N)$ Heat Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7355f6-7188-46ad-8d75-571b04985fb9",
   "metadata": {},
   "source": [
    "In this notebook, we will show how we can train a score model to perform the denoising which we showed analytically in the other notebook. The idea is that we can have a direct comparison between our ML results and the analytical results because we know the score function for the heat kernel exactly, but in more practical applications when we start with non-trivial 'training data' (i.e., give the heat equation initial conditions), we do not know the true score function for all time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ead69-4198-473d-bb1d-411d34765f60",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcb206-8144-4d5d-a8c1-549ddc6ad60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92b869-d0ac-4102-97e9-cf993e321f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from our repo\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # repo source code\n",
    "\n",
    "from src.linalg import trace, adjoint\n",
    "from src.diffusion import VarianceExpandingDiffusion, VarianceExpandingDiffusionSUN\n",
    "from src.sun import (\n",
    "    proj_to_algebra, matrix_exp, matrix_log,\n",
    "    random_sun_element, random_un_haar_element,\n",
    "    group_to_coeffs, coeffs_to_group,\n",
    "    extract_sun_algebra, embed_diag, mat_angle\n",
    ")\n",
    "from src.canon import canonicalize_sun\n",
    "from src.heat import (\n",
    "    eucl_score_hk,\n",
    "    sun_score_hk, sample_sun_hk, sun_hk, _sun_hk_unwrapped\n",
    ")\n",
    "from src.utils import grab, wrap\n",
    "from src.devices import set_device, get_device, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b716eb-71b2-4951-8a25-40e572e633c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_device('cuda', 1)\n",
    "print(summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5ccec-eabc-45db-8363-8ab7309614d1",
   "metadata": {},
   "source": [
    "## Euclidean Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc740151-46f6-4f6f-a022-3dff185441a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_heat_kernel(x, t, width=None):\n",
    "    \"\"\"Computes the Euclidean heat kernel density K(x, t) for `x` at time `t`.\"\"\"\n",
    "    if width is None:\n",
    "        width = t**0.5  # assume unit constant diffusivity\n",
    "    normalization = 1 / (2 * np.pi * width**2)**0.5\n",
    "    weight = torch.exp(-x**2 / (2 * width**2))\n",
    "    return normalization * weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f51a6-e3fe-4dcf-946f-e0d842360a44",
   "metadata": {},
   "source": [
    "Build a score net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc85fde-ee68-4f9c-9eb0-0194499f388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 8),  # data & time = 1 + 1 dims\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 1))\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        return self.net(torch.cat([x_t, t], dim=-1))\n",
    "\n",
    "\n",
    "def _test_score_net():\n",
    "    batch_size = 100\n",
    "    x = torch.randn((batch_size, 1))\n",
    "    t = torch.rand_like(x)\n",
    "    s_t = ScoreNet()(x, t)\n",
    "    print('x shape:', x.shape)\n",
    "    print('t shape:', t.shape)\n",
    "    print('s_t shape:', s_t.shape)\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        'Score output should have same shape as input'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_score_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aaf8c2-6d71-43c5-9565-24c4e0824978",
   "metadata": {},
   "source": [
    "Define the score matching loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3b66c-05e7-4ea7-84d6-7a0934b28208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_matching_loss(x_0, diffuser, score_net, tol=1e-5):\n",
    "    t = torch.rand_like(x_0)\n",
    "    t = tol + (1 - tol) * t  # stability near endpoints\n",
    "    print('t:', grab(t))\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "    \n",
    "    x_t = diffuser(x_0, t.squeeze())\n",
    "    #score = score_net(x_t, t)\n",
    "    score = score_net(x_t, t) / sigma_t\n",
    "    \n",
    "    true_score = eucl_score_hk(x_t, width=sigma_t)\n",
    "    print('score net norm:', torch.norm(score).item())\n",
    "    print('true score norm:', torch.norm(true_score).item())\n",
    "\n",
    "    #return torch.mean((score - true_score)**2)\n",
    "    return torch.mean(sigma_t**2 * (score - true_score)**2)\n",
    "\n",
    "\n",
    "def score_matching_loss(x_0, diffuser, score_net, tol=1e-4):\n",
    "    t = torch.rand_like(x_0)\n",
    "    t = tol + (1 - tol) * t  # stability near endpoints\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "    \n",
    "    # x_0 -> x_t, get s(x_t, t)\n",
    "    x_t = diffuser(x_0, t.squeeze())\n",
    "    #score = score_net(x_t, t)\n",
    "    score = score_net(x_t, t) / sigma_t\n",
    "    \n",
    "    # s(x_t, t) should approximate grad log N(x_t; x_0, sigma_t^2)\n",
    "    #true_score = eucl_score_hk(x_t, width=sigma_t)\n",
    "    #eps = -true_score * sigma_t**2\n",
    "    eps = (x_t - x_0) / sigma_t\n",
    "    return torch.mean((sigma_t * score + eps)**2)  # weight factor of sigma(t)^2 for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad3e05-fb88-4a41-90d8-b7a1c2e4f067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the training\n",
    "sigma = 1.1\n",
    "score_net = ScoreNet()\n",
    "diffuser = VarianceExpandingDiffusion(sigma)\n",
    "lr = 1e-3\n",
    "epochs = 1000\n",
    "batch_size = 1024\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "x_0 = torch.zeros((batch_size, 1))\n",
    "losses = []\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = score_matching_loss(x_0, diffuser, score_net)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {loss.item():.6f}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069aa623-5b8f-41d3-b364-1ca8c76c63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.plot(losses, lw=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e685cf-ad37-491e-b5cd-e67fa1483d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def euler_sampler(x_1, score_net, diffuser, num_steps=200, solver_type='ODE', verbose=False):\n",
    "    score_net.eval()\n",
    "    batch_size = x_1.size(0)\n",
    "    \n",
    "    trajectories = []\n",
    "    dt = 1 / num_steps\n",
    "    x_t = x_1.clone()\n",
    "    t = 1.0\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Get ODE / SDE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t, torch.tensor(t).repeat(batch_size, 1)) / sigma_t\n",
    "\n",
    "        # Integration step\n",
    "        if solver_type == 'ODE':\n",
    "            x_t = x_t + 0.5 * g_t**2 * score * dt  # ODE Euler step\n",
    "        elif solver_type == 'SDE':\n",
    "            x_t = x_t + g_t**2 * score * dt + g_t * torch.rand_like(x_t) * dt**0.5  # SDE step\n",
    "        else:\n",
    "            raise NotImplementedError(f'Integration method {solver_type} not supported')\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        trajectories.append(x_t)\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps} | x_t = {x_t.mean().item():.6f}')\n",
    "    return x_t, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db63e06-405d-4af1-b1bb-bffb3ee7f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_euclidean_denoising():\n",
    "    # Initial data: zeros\n",
    "    batch_size = 4096\n",
    "    x_0 = torch.zeros((batch_size, 1))\n",
    "    \n",
    "    # Diffuse forward: x_0 -> x_1\n",
    "    sigma = 1.1\n",
    "    diffuser = VarianceExpandingDiffusion(sigma)\n",
    "    x_1 = diffuser(x_0, t=torch.ones((batch_size,)))\n",
    "\n",
    "    # Denoise backward: x_1 -> x_0'\n",
    "    num_steps = 100\n",
    "    x_0, trajectories = euler_sampler(x_1, score_net, diffuser, num_steps, solver_type='ODE')\n",
    "    print(\"x_0':\", grab(x_0.mean().item()))\n",
    "\n",
    "    # Plot trajectories\n",
    "    times = [1.0, 0.75, 0.5, 0.25, 0.05]\n",
    "    xs = torch.linspace(-5, 5, 100)\n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "    fig.suptitle('Euclidean Reverse Denoising Process')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    for t, ax in zip(times, axes):\n",
    "        # Denoised samples\n",
    "        x_t = trajectories[int(num_steps * (1 - t))]\n",
    "        ax.hist(grab(x_t), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "\n",
    "        # Analytical heat kernel\n",
    "        hk = euclidean_heat_kernel(xs, t, width=diffuser.sigma_func(t))\n",
    "        hk /= hk.sum() * (xs[1] - xs[0])\n",
    "        ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytic HK')\n",
    "        ax.set_xlabel(r'$x_t$')\n",
    "        ax.set_title(f'$t = {t}$')\n",
    "        ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "_test_euclidean_denoising()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212d1f4-44a0-48d3-9c13-ae3245d89879",
   "metadata": {},
   "source": [
    "## ${\\rm SU}(2)$ case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d94fb-9aea-4be3-8ceb-366d6cc4be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SU2ScoreNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),  # 1 eigenangle + time = 2\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x_t, t):\n",
    "        assert len(x_t.shape) == 2, \\\n",
    "            'input eigenangles shape should be [batch_size, Nc-1]'\n",
    "        assert len(t.shape) == 1, \\\n",
    "            'times should only have a batch dimension'\n",
    "        inp = torch.cat([x_t, t.unsqueeze(-1)], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "def _test_su2_score_net():\n",
    "    batch_size = 10\n",
    "    Nc = 2\n",
    "    x = 2*np.pi*torch.rand((batch_size, 1)) - np.pi\n",
    "    t = torch.rand((batch_size,))\n",
    "    s_t = SU2ScoreNet()(x, t)\n",
    "\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        '[FAILED: score net output must have same shape as input data]'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_su2_score_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30716ef-d377-4995-9fdc-406cc4fadf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss_sun(U_0, diffuser, score_net, tol=1e-5):\n",
    "    print()\n",
    "    batch_size = U_0.size(0)\n",
    "    #t = torch.rand((batch_size,))\n",
    "    t = torch.rand((1,)) * torch.ones((batch_size,))\n",
    "    t = tol + (1 - tol) * t  # stability near endpoints\n",
    "    #t = 0.05 * torch.ones((batch_size,))  # fix t to 0.5 for testing\n",
    "    print('t:', grab(t))\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "    \n",
    "    U_t, x_t, V = diffuser.diffuse(U_0, t, n_iter=20)\n",
    "    x_t = x_t.to(dtype=t.dtype)  # TODO: fix dtype issue\n",
    "    print('x_t shape:', x_t.shape)\n",
    "    score = score_net(x_t[..., :-1], t) / sigma_t.unsqueeze(-1)  # only gives one angle\n",
    "    print('score / sigma_t shape:', score.shape)\n",
    "    \n",
    "    # s(x_t, t) should approximate grad log N(x_t; x_0, sigma_t^2)\n",
    "    #true_score = sun_score_hk(x_t[..., :-1], width=diffuser.sigma_func(0.5))  # gives 2 angles\n",
    "    true_score = sun_score_hk(x_t[..., :-1], width=diffuser.sigma_func(t))[..., :-1]\n",
    "    print('true score shape:', true_score.shape)\n",
    "    print('score net norm:', torch.norm(score).item())\n",
    "    print('true score norm:', torch.norm(true_score).item())\n",
    "    #loss = torch.mean((score - true_score)**2)\n",
    "    loss = torch.mean(sigma_t.unsqueeze(-1)**2 * (score - true_score)**2)\n",
    "    if loss.item() > 100:\n",
    "        print('=============================================================')\n",
    "        print('x_t:', x_t)\n",
    "        print('true_score:', true_score)\n",
    "        print('score net:', score)\n",
    "        print('=============================================================')\n",
    "        #loss = torch.tensor(0.0, requires_grad=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959d1cd-dafb-4c59-bffe-3f1fa9e1d0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make diffusion process\n",
    "sigma = 1.1\n",
    "score_net = SU2ScoreNet(input_dim=2, hidden_dim=64)\n",
    "diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "\n",
    "# Make \"training data\"\n",
    "batch_size = 16\n",
    "Nc = 2\n",
    "_U_0_re = torch.eye(Nc).repeat(batch_size, 1, 1)\n",
    "_U_0_im = torch.zeros((batch_size, Nc, Nc))\n",
    "U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "# Setup training hyperparams\n",
    "lr = 2e-4\n",
    "epochs = 1000\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "score_net.train()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = score_matching_loss_sun(U_0, diffuser, score_net)\n",
    "    #if loss.item() > 100:  # for now: skip the unstable outliers\n",
    "    #    continue\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {loss.item():.6f}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d53d2-efc5-4f70-ab7f-8abeeef75a9c",
   "metadata": {},
   "source": [
    "## TODO: gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63855984-7312-4ef6-ae60-b6af03220657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss vs epochs\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_yscale('log')\n",
    "ax.plot(losses, lw=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6cce7-14e9-4b22-a2e2-96cd7faf9eb1",
   "metadata": {},
   "source": [
    "Run the denoising process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11875-114f-42d8-91d9-d83fba723356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sun_gaussian(shape):\n",
    "    Nc, Nc_ = shape[-2:]\n",
    "    assert Nc == Nc_\n",
    "    return proj_to_algebra(torch.randn(shape) + 1j*torch.randn(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650bd0c-a5d4-4bad-9917-b7b4acc31e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_backwards(U_1, score_net, diffuser, num_steps=200, verbose=False, solve_type='SDE'):\n",
    "    trajectories = []\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    U_t = U_1.clone()\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Eigendecompose\n",
    "        x_t, V, V_inv = mat_angle(U_t)\n",
    "\n",
    "        # Get SDE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((U_t.size(0),))) / sigma_t\n",
    "\n",
    "        # Integration step in reverse time\n",
    "        if solve_type == 'ODE':\n",
    "            x_t = x_t + 0.5 * g_t**2 * score * dt  # ODE Euler step on spectra\n",
    "            D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "            U_t = V @ D @ V_inv\n",
    "        elif solve_type == 'SDE':\n",
    "            x_t = x_t + g_t**2 * score * dt  # SDE Euler-Maruyama drift step on spectra\n",
    "            D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "            U_t = V @ D @ V_inv\n",
    "            U_t = matrix_exp(g_t * dt**0.5 * sample_sun_gaussian(U_t.shape)) @ U_t  # SDE noise step    \n",
    "        else:\n",
    "            raise NotImplementedError(f'Integration method {solve_type} not implemented')\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        trajectories.append(U_t)\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps} | trace(U) = {trace(U_t/Nc).mean().item():.6f}')\n",
    "            print()\n",
    "\n",
    "    return U_t, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749da5f-4ca4-44d3-b8c3-d45d8e057109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data: 2x2 identity matrices\n",
    "batch_size = 4096\n",
    "Nc = 2\n",
    "_U_0_re = torch.eye(Nc).repeat(batch_size, 1, 1)\n",
    "_U_0_im = torch.zeros((batch_size, Nc, Nc))\n",
    "U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "# Diffuse: U_0 -> U_1\n",
    "U_1, _, _ = diffuser.diffuse(U_0, torch.ones(batch_size), n_iter=25)\n",
    "\n",
    "# Denoise: U_1 -> U_0'\n",
    "U_0, trajectories = denoise_backwards(U_1, score_net, diffuser, num_steps=500, solve_type='ODE')\n",
    "print(\"Re[U_0'] =\\n\", grab(U_0.mean(0)).real)\n",
    "print(\"Im[U_0'] =\\n\", grab(U_0.mean(0)).imag)\n",
    "\n",
    "# Plot reverse trajectories\n",
    "times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.05]\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "fig.suptitle('SU(2) Angular Spectral Density During Reverse Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "for t, ax in zip(times, axes):\n",
    "    # Histogram denoised samples\n",
    "    step = int((1 - t) * len(trajectories))\n",
    "    U_t = trajectories[step]\n",
    "    x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "    \n",
    "    # Analytical HK spectral density\n",
    "    hk = sun_hk(xs, width=diffuser.sigma_func(t), n_max=3)\n",
    "    hk /= hk.sum() * (xs[1] - xs[0])\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytical HK')\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c30f8-fdcc-4878-a944-47f0321c3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_0 @ adjoint(U_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10a5b6-d9e8-4e6e-935a-e1464785e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.det(U_0)*torch.linalg.det(U_0).conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c13ca-643d-4238-95c8-6665bf1514a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c8865-b31e-444e-a536-37a8c9ce2ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
