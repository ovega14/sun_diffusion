{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6804cb0e-82ee-4773-8982-97b0a0e1b0f7",
   "metadata": {},
   "source": [
    "# Euclidean Heat Kernel - Diffusion & Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a25e01-baf2-43d9-8049-3f991b0e8a28",
   "metadata": {},
   "source": [
    "In this notebook, we will show how we can\n",
    "- use the exact score function for the ${\\rm SU}(2)$ heat kernel to reverse the forward diffusion process\n",
    "- train a neural network to approximate this score function and replicate the same denoising procedure\n",
    "  \n",
    "The idea is that we can have a direct comparison between our ML results and the analytical results, since we know the score function for the heat kernel exactly.\n",
    "\n",
    "*Note: In more practical applications when we start with non-trivial 'training data' (i.e., give the heat equation initial conditions), we do not know the true score function for all time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b379ee-bdc8-4416-a054-7fea6a6b9308",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3bdde-c409-41d4-9b0a-053790543a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04966014-fa29-4081-8dd6-cda9e006c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from our repo\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # repo source code\n",
    "\n",
    "from src.linalg import trace, adjoint\n",
    "from src.diffusion import VarianceExpandingDiffusionSUN\n",
    "from src.sun import (\n",
    "    proj_to_algebra, matrix_exp, matrix_log,\n",
    "    random_sun_element, random_un_haar_element,\n",
    "    group_to_coeffs, coeffs_to_group,\n",
    "    extract_sun_algebra, embed_diag, mat_angle\n",
    ")\n",
    "from src.canon import canonicalize_sun\n",
    "from src.heat import (\n",
    "    eucl_score_hk,\n",
    "    sun_score_hk, sample_sun_hk, sun_hk, _sun_hk_unwrapped\n",
    ")\n",
    "from src.utils import grab, wrap\n",
    "from src.devices import set_device, get_device, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7d957-0069-4af7-ab5f-34efa34e60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_device('cuda', 0)\n",
    "print(summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649c97e-5e44-4844-9715-92adb496ef3b",
   "metadata": {},
   "source": [
    "## Background: ${\\rm SU}(2)$ Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc7a54-65e7-4263-b429-b9817a7d9f0e",
   "metadata": {},
   "source": [
    "The concepts of variance-expanding diffusion and denoising carry over (at a high level) from the Euclidean setting; however, there are now additional subtleties that arise from the fact that we are now formulating stochastic processes on a compact group manifold. Let's visualize these concepts using a single ${\\rm SU}(2)$ degree of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630983d-bc27-4ddb-b44b-9d9285f37b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sun_hk():\n",
    "    xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "    times = torch.linspace(0, 1, 10)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('SU(2) Spetral Heat Kernel')\n",
    "    for t in times:\n",
    "        ax.plot(grab(xs), grab(sun_hk(xs, width=t**0.5, n_max=1)), color=cmap(grab(t)))\n",
    "    time_colors = mpl.cm.ScalarMappable(mpl.colors.Normalize(times[0], times[-1]))\n",
    "    cbar = fig.colorbar(time_colors, cmap=cmap, ax=ax, label='$t$')\n",
    "    fig.show()\n",
    "\n",
    "visualize_sun_hk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf68e99-cedc-492f-b395-f5e4ea6b7151",
   "metadata": {},
   "source": [
    "We are also able to sample from the ${\\rm SU}(N)$ heat kernel, though not as easily as in the Euclidean case. Our current implementation relies on *importance sampling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4995d-36b9-4c2a-a3b6-fd84d576b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample_sun_hk():\n",
    "    batch_size = 2048\n",
    "    Nc = 2\n",
    "    _U_0_re = torch.eye(Nc).repeat(batch_size, 1, 1)\n",
    "    _U_0_im = torch.zeros((batch_size, Nc, Nc))\n",
    "    U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "    # Diffusion process\n",
    "    sigma = 1.1\n",
    "    diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "\n",
    "    times = [0.1, 0.5, 0.9, 1.0]\n",
    "    #xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)  \n",
    "    xs = torch.linspace(0, np.pi, 100).unsqueeze(-1) # `sample_sun_hk` canonicalizes angles, so better to look at [0, pi]\n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "\n",
    "    # Compare samples from the heat kernel to the analytical heat kernel at each time\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "    fig.suptitle('Samples from the SU(2) Heat Kernel over Time')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    for t, ax in zip(times, axes):\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        x_t = sample_sun_hk(batch_size, Nc=2, width=sigma_t.repeat(batch_size), n_iter=25)  # more IS iters for better sample quality at small t\n",
    "        hk = sun_hk(xs, width=sigma_t, n_max=3, eig_meas=True)\n",
    "        hk /= hk.sum() * (xs[1] - xs[0])\n",
    "        ax.hist(grab(x_t[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.55, label='HK samples')\n",
    "        ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytic SU(2) HK')\n",
    "        ax.set_xlabel(r'$\\theta$')\n",
    "        ax.set_title(f'$t = {t}$')\n",
    "        ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "test_sample_sun_hk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717edd9-fdef-4f04-9d70-93b90c52a87f",
   "metadata": {},
   "source": [
    "As opposed to sampling, we can also check that our variance-expanding diffusion process works by directly feeding in the initial configurations $U_0 = \\mathbb{I}_{2 \\times 2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e11e2-9146-4675-b6a3-6d693ae6c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sun_fwd_diffusion():\n",
    "    # Create initial data (2x2 identity matrices)\n",
    "    batch_size = 4096\n",
    "    Nc = 2\n",
    "    _U_0_re = torch.eye(Nc).repeat(batch_size, 1, 1)\n",
    "    _U_0_im = torch.zeros((batch_size, Nc, Nc))\n",
    "    U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "    # Diffusion process\n",
    "    sigma = 1.1\n",
    "    diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "\n",
    "    times = [0.1, 0.5, 0.75, 0.99]\n",
    "    xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "    fig.suptitle('SU(2) Angular Spectral Density over Forward Process')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    for t, ax in zip (times, axes):\n",
    "        # Forward diffusion\n",
    "        U_t, X_t, V = diffuser.diffuse(U_0, t=t*torch.ones((batch_size,)), n_iter=25)  # diffuse from t=0 -> t=T\n",
    "        thetas, _, _ = mat_angle(U_t)\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "\n",
    "        # Analytical HK\n",
    "        hk = sun_hk(xs, width=diffuser.sigma_func(t), n_max=5, eig_meas=True)\n",
    "        hk /= hk.sum() * (xs[1] - xs[0])\n",
    "        print(f'sigma({t}):', sigma_t.item())\n",
    "        \n",
    "        # Plot snapshots\n",
    "        ax.set_title(f'$t = {t}$')\n",
    "        ax.set_xlabel(r'$\\theta$')\n",
    "        ax.hist(grab(thetas[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.65, label='Diffused Samples')\n",
    "        ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytical HK')\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "visualize_sun_fwd_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887b5cf-b0c0-4129-9a80-0ebeed22d0b9",
   "metadata": {},
   "source": [
    "## Denoising with the Exact Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be7800-7687-4670-9dff-b2af378ae225",
   "metadata": {},
   "source": [
    "We have also computed the exact score function for the ${\\rm SU}(N)$ heat kernel. Though it is no longer a simple linear function of the argument as it was for the Euclidean heat kernel, we may still use it to solve the reverse ODE/SDE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a701b3c-231b-4526-83e0-07121b24c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sun_gaussian(shape):\n",
    "    \"\"\"Helper func to sample Gaussian matrix in the Lie algebra su(Nc).\"\"\"\n",
    "    Nc, Nc_ = shape[-2:]\n",
    "    assert Nc == Nc_\n",
    "    return proj_to_algebra(torch.randn(shape) + 1j*torch.randn(shape))\n",
    "\n",
    "\n",
    "def denoise_backwards(U_1, diffuser, num_steps=200, verbose=False, solve_type='SDE'):\n",
    "    trajectories = []\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    U_t = U_1.clone()\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Eigendecompose\n",
    "        x_t, V, V_inv = mat_angle(U_t)\n",
    "\n",
    "        # Get SDE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = sun_score_hk(x_t[..., :-1], width=sigma_t)\n",
    "\n",
    "        # Integration step in reverse time\n",
    "        if solve_type == 'ODE':\n",
    "            x_t = x_t + 0.5 * g_t**2 * score * dt  # ODE Euler step on spectra\n",
    "            D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "            U_t = V @ D @ V_inv\n",
    "        elif solve_type == 'SDE':\n",
    "            x_t = x_t + g_t**2 * score * dt  # SDE Euler-Maruyama drift step on spectra\n",
    "            D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "            U_t = V @ D @ V_inv\n",
    "            U_t = matrix_exp(g_t * dt**0.5 * sample_sun_gaussian(U_t.shape)) @ U_t  # SDE noise step    \n",
    "        else:\n",
    "            raise NotImplementedError(f'Integration method {solve_type} not implemented')\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        trajectories.append(U_t)\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps} | trace(U) = {trace(U_t/Nc).mean().item():.6f}')\n",
    "            print()\n",
    "\n",
    "    return U_t, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355c2f2-70b6-4d9e-a7c3-6a8e96282fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data: 2x2 identity matrices\n",
    "num_samples = 1024\n",
    "Nc = 2\n",
    "_U_0_re = torch.eye(Nc).repeat(num_samples, 1, 1)\n",
    "_U_0_im = torch.zeros((num_samples, Nc, Nc))\n",
    "U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "# Diffuse: U_0 -> U_1\n",
    "sigma = 1.1\n",
    "diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "U_1, _, _ = diffuser.diffuse(U_0, torch.ones(num_samples), n_iter=10)\n",
    "    \n",
    "# Denoise: U_1 -> U_0'\n",
    "U_0, trajectories = denoise_backwards(U_1, diffuser, num_steps=500, solve_type='SDE')\n",
    "print(\"Re[U_0'] =\\n\", grab(U_0.mean(0)).real)\n",
    "print(\"Im[U_0'] =\\n\", grab(U_0.mean(0)).imag)\n",
    "\n",
    "# Plot reverse trajectories\n",
    "times = [1.0, 0.75, 0.25, 0.05]\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "fig.suptitle('SU(2) Angular Spectral Density During Reverse Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "for t, ax in zip(times, axes):\n",
    "    # Histogram denoised samples\n",
    "    step = int((1 - t) * len(trajectories))\n",
    "    U_t = trajectories[step]\n",
    "    x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "    \n",
    "    # Analytical HK spectral density\n",
    "    hk = sun_hk(xs, width=diffuser.sigma_func(t), n_max=3)\n",
    "    hk /= hk.sum() * (xs[1] - xs[0])\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytical HK')\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115fba6-725b-4f59-a283-dc54557452f6",
   "metadata": {},
   "source": [
    "## Denoising with a Trained Score Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45551a30-2ac1-4a10-bbdc-3fa79844f267",
   "metadata": {},
   "source": [
    "First we build a very simple MLP as our score network, which will input both $x$ and $t$ (for now, no special embeddings for $t$, just handle the raw time as-is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2471cbe-9aaf-44a4-8d84-34093d36199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SU2ScoreNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),  # 1 eigenangle + time = 2\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x_t, t):\n",
    "        assert len(x_t.shape) == 2, \\\n",
    "            'input eigenangles shape should be [batch_size, Nc-1]'\n",
    "        assert len(t.shape) == 1, \\\n",
    "            'times should only have a batch dimension'\n",
    "        inp = torch.cat([x_t, t.unsqueeze(-1)], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "def _test_su2_score_net():\n",
    "    batch_size = 10\n",
    "    Nc = 2\n",
    "    x = 2*np.pi*torch.rand((batch_size, 1)) - np.pi\n",
    "    t = torch.rand((batch_size,))\n",
    "    s_t = SU2ScoreNet()(x, t)\n",
    "\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        '[FAILED: score net output must have same shape as input data]'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_su2_score_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82249ff-0864-4acc-b998-98cebeb53980",
   "metadata": {},
   "source": [
    "Define the modified **score matching** loss adapted to the ${\\rm SU}(N)$ group manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd01c7b-22b7-441b-9086-f46c8f17c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss_sun(U_0, diffuser, score_net, tol=1e-53):\n",
    "    batch_size = U_0.size(0)\n",
    "    t = torch.rand((batch_size,))\n",
    "    t = tol + (1 - tol) * t  # stability near endpoints\n",
    "    \n",
    "    U_t, x_t, V = diffuser.diffuse(U_0, t, n_iter=10)\n",
    "    x_t = x_t.to(dtype=t.dtype)  # TODO: fix dtype issue\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "    score = score_net(x_t[..., :-1], t) / sigma_t.unsqueeze(-1)\n",
    "    \n",
    "    true_score = sun_score_hk(x_t[..., :-1], width=diffuser.sigma_func(t))[..., :-1]\n",
    "    loss = torch.mean(sigma_t.unsqueeze(-1)**2 * (score - true_score)**2)\n",
    "    if loss.item() > 100:\n",
    "        print('=============================================================')\n",
    "        print('x_t:', x_t)\n",
    "        print('true_score:', true_score)\n",
    "        print('score net:', score)\n",
    "        print('=============================================================')\n",
    "        #loss = torch.tensor(0.0, requires_grad=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616f317-7e8a-490d-9f41-71930b8fb843",
   "metadata": {},
   "source": [
    "Now do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c1dd9-38ac-4489-ab35-0e726b34b524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make diffusion process\n",
    "sigma = 1.1\n",
    "score_net = SU2ScoreNet(input_dim=2, hidden_dim=64)\n",
    "diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "\n",
    "# Make \"training data\"\n",
    "batch_size = 256\n",
    "Nc = 2\n",
    "_U_0_re = torch.eye(Nc).repeat(batch_size, 1, 1)\n",
    "_U_0_im = torch.zeros((batch_size, Nc, Nc))\n",
    "U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "# Setup training hyperparams\n",
    "lr = 1e-3\n",
    "epochs = 1000\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "score_net.train()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = score_matching_loss_sun(U_0, diffuser, score_net)\n",
    "    torch.nn.utils.clip_grad_norm_(score_net.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "    if loss.item() > 100:  # FOR NOW: skip the unstable outliers that blow up due to x near +/- pi\n",
    "        continue\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {loss.item():.6f}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea3c52-2c85-4fc8-898b-8f0bcb23fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.plot(losses, lw=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2279a6-063a-41f1-8382-7434cea4887a",
   "metadata": {},
   "source": [
    "To generate new samples, we define a function to sample from the diffusion model posterior using our trained score network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06206ec1-992d-4cf5-8cfa-8afbe6739fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def euler_sampler(U_1, score_net, diffuser, num_steps=200, solve_type='SDE', verbose=False):\n",
    "    trajectories = []\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    U_t = U_1.clone()\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Eigendecompose\n",
    "        x_t, V, V_inv = mat_angle(U_t)\n",
    "\n",
    "        # Get SDE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((U_t.size(0),))) / sigma_t\n",
    "\n",
    "        # Integration step in reverse time\n",
    "        if solve_type == 'ODE':\n",
    "            x_t = x_t + 0.5 * g_t**2 * score * dt  # ODE Euler step on spectra\n",
    "            D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "            U_t = V @ D @ V_inv\n",
    "        elif solve_type == 'SDE':\n",
    "            x_t = x_t + g_t**2 * score * dt  # SDE Euler-Maruyama drift step on spectra\n",
    "            D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "            U_t = V @ D @ V_inv\n",
    "            U_t = matrix_exp(g_t * dt**0.5 * sample_sun_gaussian(U_t.shape)) @ U_t  # SDE noise step    \n",
    "        else:\n",
    "            raise NotImplementedError(f'Integration method {solve_type} not implemented')\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        trajectories.append(U_t)\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps} | trace(U) = {trace(U_t/Nc).mean().item():.6f}')\n",
    "            print()\n",
    "\n",
    "    return U_t, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d6bcb-1c48-4d38-826d-eb34b622eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data: 2x2 identity matrices\n",
    "num_samples = 2048\n",
    "Nc = 2\n",
    "_U_0_re = torch.eye(Nc).repeat(num_samples, 1, 1)\n",
    "_U_0_im = torch.zeros((num_samples, Nc, Nc))\n",
    "U_0 = torch.complex(_U_0_re, _U_0_im)\n",
    "\n",
    "# Diffuse: U_0 -> U_1\n",
    "U_1, _, _ = diffuser.diffuse(U_0, torch.ones(num_samples), n_iter=10)\n",
    "\n",
    "# Denoise: U_1 -> U_0'\n",
    "U_0, trajectories = euler_sampler(U_1, score_net, diffuser, num_steps=500, solve_type='ODE')\n",
    "print(\"Re[U_0'] =\\n\", grab(U_0.mean(0)).real)\n",
    "print(\"Im[U_0'] =\\n\", grab(U_0.mean(0)).imag)\n",
    "# TODO: check U_0 unitarity\n",
    "\n",
    "# Plot reverse trajectories\n",
    "times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.05]\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "fig.suptitle('SU(2) Angular Spectral Density During Reverse Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "for t, ax in zip(times, axes):\n",
    "    # Histogram denoised samples\n",
    "    step = int((1 - t) * len(trajectories))\n",
    "    U_t = trajectories[step]\n",
    "    x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "    \n",
    "    # Analytical HK spectral density\n",
    "    hk = sun_hk(xs, width=diffuser.sigma_func(t), n_max=3)\n",
    "    hk /= hk.sum() * (xs[1] - xs[0])\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytical HK')\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45a56a-8a7f-413e-95f9-3b519d39153f",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- asymmetry in generated angle spectrum... should be mirror symmetric. perhaps because I cut off the last entry of score net?\n",
    "- small time training less stable, or at least the results look worse there\n",
    "- even for arbitrary time, we sometimes sample pi or -pi during fwd diffusion, which causes K(x, t) to blow up. need a fix\n",
    "    - right now: just ignoring the epochs where loss blows up\n",
    "    - consider limiting the samples away from +/- pi?\n",
    "    - more general: try to stabilize the grad(K) / K calculation in analytical SU(N) HK score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2980ee-ab06-40f3-836b-c4b24108527d",
   "metadata": {},
   "source": [
    "We also check the special unitarity of our generated results. Namely, that\n",
    "- $U^\\dagger U = \\mathbb{I}$\n",
    "- $\\det U = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8c44e-1ac0-4c46-878f-2330fc613db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check special\n",
    "detU = (torch.linalg.det(U_0) * torch.linalg.det(U_0).conj()).real **0.5\n",
    "print('det(U) =', grab(detU))\n",
    "assert torch.allclose(detU, torch.ones(num_samples)), \\\n",
    "    '[FAILED: U does NOT have unit determinant]'\n",
    "print('[PASSED test of unit determinant]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b789d79-3208-4fb8-9dd6-9437b0cf271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unitary\n",
    "UUdag = U_0 @ adjoint(U_0)\n",
    "print('U @ U^dagger =', grab(UUdag))\n",
    "assert torch.allclose(UUdag, torch.eye(Nc).repeat(num_samples, 1, 1).to(U_0), atol=1e-4), \\\n",
    "    '[FAILED: U is NOT unitary]'\n",
    "print('[PASSED test of unitarity]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dbd52-9633-44b7-9204-ad2b77f76ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
