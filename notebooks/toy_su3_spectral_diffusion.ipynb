{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d6ca7-f107-4624-a0de-e19baf4e67ca",
   "metadata": {},
   "source": [
    "# Spectral Diffusion for a Toy ${\\rm SU}(3)$ Model using Group-Valued Score Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea60c33-70af-4bf1-b975-e186784033a1",
   "metadata": {},
   "source": [
    "In this notebook, we apply the same machinery we used in the other notebook (where we trained a score network to learn the heat kernel and denoise the variance-expanding diffusion process) to learn a toy theory involving a single ${\\rm SU}(3)$ matrix $U$ (one independent eigenangle $\\theta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6096b-1be2-4312-b2fe-52a4b3fe25ce",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a312c-db55-4997-a9cd-9ab97c8c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c8056-2c0b-40dc-8076-40fad7bc5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sun_diffusion\n",
    "import sun_diffusion.analysis as al  # analysis library\n",
    "\n",
    "from sun_diffusion.action import SUNToyPolynomialAction\n",
    "from sun_diffusion.sun import (\n",
    "    mat_angle, adjoint,\n",
    "    extract_diag, embed_diag,\n",
    "    random_sun_element, random_un_haar_element,\n",
    ")\n",
    "from sun_diffusion.diffusion import VarianceExpandingDiffusionSUN, PowerDiffusionSUN\n",
    "from sun_diffusion.heat import sun_score_hk\n",
    "from sun_diffusion.utils import grab, compute_ess, compute_kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbeaf2-083f-49fb-ace6-2ed14d37d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sun_diffusion.devices import get_device, get_dtype, set_device, summary\n",
    "set_device('cuda', 0)\n",
    "print(summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c31ee1-6700-4dc7-aa2d-2e9b6918e69f",
   "metadata": {},
   "source": [
    "## Define a Target Theory and Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0531e4-270f-4264-b811-0c15c90751d9",
   "metadata": {},
   "source": [
    "The target theory whose distribution we will try to reproduce is specified by a toy action of our choosing, which we define to be \n",
    "$$S_i(U) = -\\frac{\\beta}{2} {\\rm Re}{\\rm Tr}\\left[\\sum_n c_n U^n\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223b0c6-14c7-4913-9d71-72565d689fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 3\n",
    "beta = 9.0\n",
    "coeffs_dict = {\n",
    "    0: [1.0, 0., 0.],\n",
    "    1: [0.17, -0.65, 1.22],\n",
    "    2: [0.98, -0.63, -0.21]\n",
    "}  # from Table I of [2008.05456]\n",
    "\n",
    "coeffs = 1  # change this to try other coefficient sets\n",
    "action = SUNToyPolynomialAction(beta, coeffs_dict[coeffs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058173a7-080b-45f9-a955-df152e9fa135",
   "metadata": {},
   "source": [
    "To generate configurations, we will use the Metropolis algorithm for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02599f4b-8d5a-401f-a798-98e0f341a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_metropolis(batch_size, Nc, action, num_therm, num_iters, step_size, save_freq=10):\n",
    "    \"\"\"Batched Metropolis sampler.\"\"\"\n",
    "    action_vals = []\n",
    "    accept_rates = []\n",
    "    \n",
    "    U = random_sun_element(batch_size, Nc=Nc)\n",
    "    ens = []\n",
    "    for i in tqdm.tqdm(range(-num_therm, num_iters)):\n",
    "        # Proposal\n",
    "        V = random_sun_element(batch_size, Nc=Nc, scale=step_size)\n",
    "        Up = V @ U\n",
    "        dS = action(Up) - action(U)\n",
    "\n",
    "        # Accept / Reject\n",
    "        r = torch.rand(batch_size)  # accept w/ prob = exp(-dS)\n",
    "        accept_mask = (r < torch.exp(-dS))[:, None, None]\n",
    "        U = torch.where(accept_mask, Up, U)\n",
    "\n",
    "        action_vals.append(grab(action(U).mean()))\n",
    "        accept_rates.append(grab(torch.sum(accept_mask) / batch_size))\n",
    "        if i >= 0 and (i+1) % save_freq == 0:\n",
    "            ens.append(U)\n",
    "    return torch.cat(ens), action_vals, accept_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef48d5-b65c-43e0-b6be-f4455ee1c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "batch_size = 32\n",
    "num_therm = 1_000\n",
    "num_iters = 10_240\n",
    "save_freq = 10\n",
    "step_size = 0.4\n",
    "\n",
    "num_train = batch_size * (num_iters // save_freq)\n",
    "print(f'{num_train=}')\n",
    "\n",
    "U_train, action_vals, accept_rates = apply_metropolis(\n",
    "    batch_size = batch_size,\n",
    "    Nc = Nc,\n",
    "    action = action,\n",
    "    num_therm = num_therm,\n",
    "    num_iters = num_iters,\n",
    "    step_size = step_size\n",
    ")\n",
    "print('U_train shape:', U_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30358ff4-d9c5-4e3f-a7fb-eaa2ae243a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Metropolis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Metropolis Steps')\n",
    "\n",
    "axes[0].plot(*al.bin_data(action_vals, binsize=100))\n",
    "axes[0].set_ylabel('Average Action')\n",
    "\n",
    "axes[1].plot(*al.bin_data(accept_rates, binsize=100))\n",
    "axes[1].set_ylabel('Acceptance Rate')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026b18a-5a1e-47ef-802b-b71da1d6dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper funcs for plotting SU(3) spectral densities\n",
    "from sun_diffusion.heat import _sun_hk_meas_D, _sun_hk_meas_J\n",
    "\n",
    "def log_meas(thetas):\n",
    "    Nc = thetas.shape[-1]\n",
    "    delta = torch.stack([\n",
    "        thetas[..., i] - thetas[..., j]\n",
    "        for i in range(Nc) for j in range(i+1, Nc)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    logJ = torch.sum(\n",
    "        _sun_hk_meas_D(delta).abs().log() +\n",
    "        _sun_hk_meas_J(delta).abs().log(),\n",
    "    dim=-1)\n",
    "    return logJ\n",
    "\n",
    "\n",
    "# All 6 Weyl permutations for SU(3)\n",
    "_SU3_WYL_PERMS = torch.tensor([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 2],\n",
    "    [0, 2, 1],\n",
    "    [2, 0, 1],\n",
    "    [1, 2, 0],\n",
    "    [2, 1, 0],\n",
    "])\n",
    "\n",
    "def random_weyl_permute(thetas, eigvecs=None):\n",
    "    batch_shape = thetas.shape[:-1]\n",
    "    n_batch = thetas.numel() // 3\n",
    "\n",
    "    # flatten batch\n",
    "    thetas_flat = thetas.reshape(n_batch, 3)\n",
    "    if eigvecs is not None:\n",
    "        eigvecs_flat = eigvecs.reshape(n_batch, 3, 3)\n",
    "\n",
    "    # random perms for each batch element\n",
    "    idx = torch.randint(0, 6, (n_batch,))\n",
    "    perms = _SU3_WYL_PERMS[idx]  # shape (n_batch, 3)\n",
    "\n",
    "    # apply permutation to eigenangles\n",
    "    thetas_perm = torch.stack([thetas_flat[i, perms[i]] for i in range(n_batch)], dim=0)\n",
    "    if eigvecs is not None:\n",
    "        # apply same permutation to columns of eigenvectors\n",
    "        eigvecs_perm = torch.stack([eigvecs_flat[i, :, perms[i]] for i in range(n_batch)], dim=0)\n",
    "        thetas_perm = thetas_perm.reshape(*batch_shape, 3)\n",
    "        eigvecs_perm = eigvecs_perm.reshape(*eigvecs.shape)\n",
    "        return thetas_perm, eigvecs_perm\n",
    "    return thetas_perm.reshape(*batch_shape, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffff9d-0138-4a5d-af2d-07398fa1d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range = [[-np.pi, np.pi], [-np.pi, np.pi]]\n",
    "\n",
    "grid = 100\n",
    "th1 = torch.linspace(-np.pi, np.pi, grid)\n",
    "th2 = torch.linspace(-np.pi, np.pi, grid)\n",
    "theta1, theta2 = torch.meshgrid(th1, th2, indexing='ij')\n",
    "theta3 = - (theta1 + theta2)\n",
    "thetas = torch.stack([theta1, theta2, theta3], dim=-1)  # [grid, grid, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharey=True)\n",
    "axes[0].set_ylabel(r'$\\theta_2$')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(r'$\\theta_1$')\n",
    "    ax.set_aspect(1.0)\n",
    "\n",
    "# Metropolis samples\n",
    "x_train, _, _ = mat_angle(U_train)\n",
    "x_train = random_weyl_permute(x_train)  # random permutations for Weyl symmetry\n",
    "x_train = grab(x_train)\n",
    "axes[0].set_title('Metropolis')\n",
    "axes[0].hist2d(x_train[:, 0], x_train[:, 1], bins=80, density=True, range=hist_range)\n",
    "\n",
    "# Target density\n",
    "axes[1].set_title('Target')\n",
    "log_haar = log_meas(thetas)  # for plots w.r.t. Lebesgue measure\n",
    "logp = -action.value_eigs(thetas)\n",
    "logp -= logp.max()  # for stability\n",
    "p_target = torch.exp(logp + log_haar)\n",
    "axes[1].contourf(grab(theta1), grab(theta2), grab(p_target), levels=40, cmap='viridis')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6e056-a2dd-44f0-9932-5fa43e80f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = PowerDiffusionSUN(kappa=3.0, alpha=1.0)\n",
    "times = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "hist_range = [[-np.pi, np.pi], [-np.pi, np.pi]]\n",
    "fig, axes = plt.subplots(1, len(times) + 1, sharey=True, figsize=(4*len(times) + 4, 4))\n",
    "axes[0].set_ylabel(r'$\\theta_2$')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(r'$\\theta_1$')\n",
    "    ax.set_aspect(1.0)\n",
    "\n",
    "# Diffused histograms\n",
    "U_0 = U_train.clone()#[::10]\n",
    "for t, ax in zip(times, axes[:-1]):\n",
    "    if t == 0:  # avoid sampling from HK at t=0\n",
    "        x_t, _, _ = mat_angle(U_0)\n",
    "    else:\n",
    "        U_t, _, _ = diffuser.diffuse(U_0, t*torch.ones(U_0.size(0)), n_iter=15)\n",
    "        x_t, _, _ = mat_angle(U_t)\n",
    "    x_t = random_weyl_permute(x_t)\n",
    "    ax.hist2d(grab(x_t[:, 0]), grab(x_t[:, 1]), bins=50, density=True, range=hist_range)\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "\n",
    "# Haar Uniform density\n",
    "p_haar = torch.exp(log_haar)\n",
    "axes[-1].contourf(grab(theta1), grab(theta2), grab(p_haar), levels=40, cmap='viridis')\n",
    "axes[-1].set_title('Haar Uniform')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc9f7c-2707-479f-bed3-39ee349f08f5",
   "metadata": {},
   "source": [
    "## Train a Score Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ff214-9f53-42d8-ac52-5c71797e053a",
   "metadata": {},
   "source": [
    "Now we must construct a score network that will take as input the eigenangle $\\theta$ and time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bb47e-3908-424a-b908-000451af94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SU3ScoreNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.nk = (input_dim - 1)//2\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, 2))\n",
    "        \n",
    "    def forward(self, x_t, t):\n",
    "        assert len(x_t.shape) == 2, \\\n",
    "            'input eigenangles shape should be [batch_size, Nc-1]'\n",
    "        assert len(t.shape) == 1, \\\n",
    "            'times should only have a batch dimension'\n",
    "        cos_t = torch.cos(torch.exp(-10.0*(torch.arange(1,self.nk+1)/self.nk - 0.5))*t.unsqueeze(-1))\n",
    "        sin_t = torch.sin(torch.exp(-10.0*(torch.arange(1,self.nk+1)/self.nk - 0.5))*t.unsqueeze(-1))\n",
    "        inp = torch.cat([x_t, cos_t, sin_t], dim=-1)\n",
    "        return self.net(inp) * torch.sin(x_t) # enforce score -> 0 at endpoints\n",
    "\n",
    "\n",
    "def _test_su3_score_net():\n",
    "    batch_size = 10\n",
    "    Nc = 3\n",
    "    \n",
    "    x1 = 2*np.pi*torch.rand((batch_size, 1)) - np.pi\n",
    "    x2 = 2*np.pi*torch.rand((batch_size, 1)) - np.pi\n",
    "    x = torch.cat([x1, x2], dim=-1)\n",
    "    print('x shape:', x.shape)\n",
    "    t = torch.rand((batch_size,))\n",
    "    s_t = SU3ScoreNet(input_dim=6)(x, t)\n",
    "    print('score shape:', s_t.shape)\n",
    "\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        '[FAILED: score net output must have same shape as input data]'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_su3_score_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4343631-6c53-4658-af5b-b290dfaa8273",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11c5d4-00fd-4d13-9cf2-e5484542d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss_sun(U_0, diffuser, score_net, tol=1e-3):\n",
    "    batch_size = U_0.size(0)\n",
    "    t = tol + (1 - tol) * torch.rand(batch_size)\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "\n",
    "    U_t, xs, V = diffuser.diffuse(U_0, t, n_iter=25)\n",
    "    x_t, P, Pinv = mat_angle(U_t)\n",
    "\n",
    "    # Random permutation so score net seels all Weyl chambers\n",
    "    x_t, P = random_weyl_permute(x_t, P)\n",
    "    Pinv = adjoint(P)\n",
    "\n",
    "    score = score_net(x_t[..., :-1], t)\n",
    "    # NOTE(gkanwar): Passing xs instead of (x_t - x_0) is important here\n",
    "    true_score_xs = sun_score_hk(xs[..., :-1], width=sigma_t)\n",
    "    true_score = extract_diag(Pinv @ V @ embed_diag(true_score_xs).to(V) @ adjoint(V) @ P).real[...,:-1]\n",
    "\n",
    "    # NOTE(gkanwar): Scaling by sigma_t^2 results in a roughly constant variance,\n",
    "    # making training much more stable.\n",
    "    diff = score - true_score\n",
    "    loss = torch.mean(sigma_t[:, None]**2 * diff**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f0c2-eff4-4088-ab04-8a86904a35f4",
   "metadata": {},
   "source": [
    "Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90fe27-8301-4a46-8b01-6d9871b52b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Make diffusion process\n",
    "kappa = 3.0\n",
    "alpha = 1.0\n",
    "diffuser = PowerDiffusionSUN(kappa, alpha)\n",
    "#diffuser = VarianceExpandingDiffusionSUN(kappa)\n",
    "\n",
    "score_net = SU3ScoreNet(input_dim=52, hidden_dim=64)\n",
    "\n",
    "# Setup training hyperparams\n",
    "epochs = 100\n",
    "lr = 1e-2\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Prepare dataloader\n",
    "batch_size = 3072\n",
    "dataset = TensorDataset(U_train) \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=get_device()))\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "score_net.train()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (U_0_batch,) in enumerate(tqdm.tqdm(dataloader, leave=False)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = score_matching_loss_sun(U_0_batch, diffuser, score_net)\n",
    "        torch.nn.utils.clip_grad_norm_(score_net.parameters(), max_norm=10.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += grab(loss)\n",
    "        losses.append(grab(loss))\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb9d44-9413-4741-8eca-f93fcb4923cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss vs epochs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2.5))\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "xs, ys = al.bin_data(losses, binsize=25)\n",
    "xs = xs / (num_train/batch_size) # convert to epochs\n",
    "ax.plot(xs, ys, lw=0.75)\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e527f41-80d1-498f-8949-9f5c40098ed4",
   "metadata": {},
   "source": [
    "## Denoising Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b92cd-adab-4a8a-a98e-52827de40b77",
   "metadata": {},
   "source": [
    "Now we run the reverse process to generate new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f004c1-b6c1-49eb-b243-87a17602e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sun_gaussian(shape):\n",
    "    Nc, Nc_ = shape[-2:]\n",
    "    assert Nc == Nc_\n",
    "    return proj_to_algebra(torch.randn(shape) + 1j*torch.randn(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4b92d-81bc-4060-b9e5-a85664376925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_haar_su3(x):\n",
    "    \"\"\"\n",
    "    Computes log Haar density for SU(3) using the 2 independent eigenangles.\n",
    "    x: tensor of shape [batch_size, 2], containing angles theta1, theta2\n",
    "    \"\"\"\n",
    "    theta1 = x[:, 0]\n",
    "    theta2 = x[:, 1]\n",
    "\n",
    "    term1 = torch.log(torch.abs(torch.sin((theta1 - theta2)/2))**2)\n",
    "    term2 = torch.log(torch.abs(torch.sin((2*theta1 + theta2)/2))**2)\n",
    "    term3 = torch.log(torch.abs(torch.sin((theta1 + 2*theta2)/2))**2)\n",
    "\n",
    "    log_haar = term1 + term2 + term3\n",
    "\n",
    "    # Approximate normalization constant for SU(3)\n",
    "    log_norm = math.log((2 * math.pi)**2 / 3)  # optional, depends on your usage\n",
    "    return log_haar - log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad594ece-104e-4fe5-ad26-6046fd58610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_reverse_ODE_eigs(U_1, logr, score_net, diffuser, num_steps=200, verbose=False):\n",
    "    trajectories = {\n",
    "        'U_t': [],\n",
    "        'x_t': [],\n",
    "        'logp': [],\n",
    "        'logq': [],\n",
    "        'kl_div': [],\n",
    "        'ess': [],\n",
    "        't': [],\n",
    "        'Z': [],\n",
    "    }\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    batch_size = U_1.size(0)\n",
    "    \n",
    "    U_t = U_1.clone()\n",
    "    x_t, V, V_inv = mat_angle(U_t)\n",
    "    logJ = 0.\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        x_t, V, V_inv = mat_angle(U_t)\n",
    "        x_t = random_weyl_permute(x_t)\n",
    "        \n",
    "        # Get ODE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((batch_size,)))\n",
    "        jac = torch.func.vmap(torch.func.jacfwd(lambda x: score_net(x[None], torch.tensor(t)[None])[0]))(x_t[..., :-1])\n",
    "        div = torch.einsum('...ii->...', jac)\n",
    "        \n",
    "        # Integration step in reverse time\n",
    "        score = torch.cat([score, -score.sum(-1, keepdim=True)], dim=-1)\n",
    "        x_t = x_t + 0.5 * g_t**2 * score * dt\n",
    "        logJ = logJ + 0.5 * g_t**2 * div * dt\n",
    "\n",
    "        # Eigen-recomposition\n",
    "        D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "        U_t = V @ D @ V_inv\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        logp = -action(U_t) + log_haar_su3(x_t)\n",
    "        logq = logr - logJ\n",
    "        Z = al.bootstrap(grab((logp - logq).exp()), Nboot=1000, f=al.rmean)\n",
    "        kl_div = al.bootstrap(grab(logp), grab(logq), Nboot=1000, f=compute_kl_div)\n",
    "        ess = al.bootstrap(grab(logp), grab(logq), Nboot=1000, f=compute_ess)\n",
    "        if verbose and step % 20 == 0:\n",
    "            print(f'Step {step}/{num_steps}')\n",
    "            print('logp =', logp.mean().item())\n",
    "            print('logq =', logq.mean().item())\n",
    "            print('Dkl =', kl_div)\n",
    "            print('ESS =', ess)\n",
    "            print()\n",
    "        trajectories['t'].append(t)\n",
    "        trajectories['x_t'].append(grab(x_t))\n",
    "        trajectories['U_t'].append(grab(U_t))\n",
    "        trajectories['logp'].append(al.bootstrap(grab(logp), Nboot=1000, f=al.rmean))\n",
    "        trajectories['logq'].append(al.bootstrap(grab(logq), Nboot=1000, f=al.rmean))\n",
    "        trajectories['kl_div'].append(kl_div)\n",
    "        trajectories['ess'].append(ess)\n",
    "        trajectories['Z'].append(Z)\n",
    "\n",
    "    for key in ['Z', 'kl_div', 'ess', 'logp', 'logq']:\n",
    "        trajectories[key] = np.stack(trajectories[key], axis=1)\n",
    "    \n",
    "    return U_t, logJ, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca990509-539a-411d-a064-988ef9e6613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples = batch_size\n",
    "num_samples = 4096\n",
    "Nc = 3\n",
    "U_1 = random_un_haar_element(num_samples, Nc=Nc)\n",
    "U_1 *= (torch.linalg.det(U_1)**(-1/Nc) * torch.exp(2j*np.pi*torch.randint(Nc, size=(num_samples,))/Nc))[...,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d656e4-edce-4e72-954a-3e627bd65980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get prior log likelihood\n",
    "x_1, _, _ = mat_angle(U_1)\n",
    "logr = log_haar_su3(x_1)\n",
    "print('avg logr =', logr.mean().item())\n",
    "print('std logr =', logr.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74634ca8-29d6-422d-b8b4-00a528e001b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U_0, logJ, history = solve_reverse_ODE_eigs(U_1, logr, score_net, diffuser, num_steps=200, verbose=True)\n",
    "x_0, _, _ = mat_angle(U_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2bd317-6c1c-4444-bc8e-26228f1443dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reverse trajectories\n",
    "times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.01]\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(3*len(times), 4), sharey=True)\n",
    "fig.suptitle('SU(3) Angular Spectral Density During Reverse Process')\n",
    "for t, ax in zip(times, axes):\n",
    "    # Histogram denoised samples\n",
    "    step = int((1 - t) * len(history['U_t']))\n",
    "    # U_t = torch.tensor(history['U_t'][step])\n",
    "    # x_t, _, _ = mat_angle(U_t)\n",
    "    # x_t = random_weyl_permute(x_t)\n",
    "    x_t = torch.tensor(history['x_t'][step])\n",
    "    ax.hist2d(grab(x_t[:, 0]), grab(x_t[:, 1]), bins=50, density=True)\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xlabel(r'$\\theta_1$')\n",
    "    ax.set_ylabel(r'$\\theta_2$')\n",
    "    ax.set_aspect(1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c51c5-73fa-4d24-b1f9-b219f57e0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reconstruct_logq_from_score(score_net, thetas, ti=0.0, num_steps=200):\n",
    "    \"\"\"compute logq integrating score ODE from ti -> 1\"\"\"\n",
    "    assert len(thetas.shape) == 2, 'thetas must have shape (bs, Nc)'\n",
    "    batch_size = thetas.shape[0]\n",
    "    logJ = torch.zeros_like(thetas[...,0])\n",
    "    dt = (1-ti)/num_steps\n",
    "    t = ti\n",
    "    x_t = thetas\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Get ODE params\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((batch_size,)))\n",
    "        jac = torch.func.vmap(torch.func.jacfwd(lambda x: score_net(x[None], torch.tensor(t)[None])[0]))(x_t[..., :-1])\n",
    "        div = torch.einsum('...ii->...', jac)\n",
    "\n",
    "        # Integration step in reverse time\n",
    "        score = torch.cat([score, -score.sum(-1, keepdim=True)], dim=-1)\n",
    "        x_t = x_t - 0.5 * g_t**2 * score * dt\n",
    "        logJ = logJ + 0.5 * g_t**2 * div * dt\n",
    "\n",
    "        t += dt\n",
    "    assert np.isclose(t, 1.0)\n",
    "    logr = log_haar_su3(x_t)\n",
    "    return logr - logJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163fbaa-29a8-4a44-825b-03f2b5c8e9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 100\n",
    "theta1 = torch.linspace(-np.pi, np.pi, grid)\n",
    "theta2 = torch.linspace(-np.pi, np.pi, grid)\n",
    "Theta1, Theta2 = torch.meshgrid(theta1, theta2, indexing=\"ij\")\n",
    "Theta3 = -Theta1 - Theta2\n",
    "thetas = torch.stack([Theta1, Theta2, Theta3], dim=-1)  # [grid, grid, 3]\n",
    "\n",
    "thetas_flat = thetas.reshape(-1, thetas.shape[-1])\n",
    "logq = reconstruct_logq_from_score(score_net, thetas_flat, ti=0.0, num_steps=200)\n",
    "logq = logq.reshape((grid, grid))\n",
    "logq -= logq.max()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contourf(\n",
    "    grab(Theta1),\n",
    "    grab(Theta2),\n",
    "    grab(logq.exp()),\n",
    "    levels=40,\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.xlabel(r'$\\theta_1$')\n",
    "plt.ylabel(r'$\\theta_2$')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title('Learned density')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653af20-b4b2-42d5-8a79-e5eecbdea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(16, 4))\n",
    "\n",
    "axes[0].errorbar(history['t'], history['logp'][0], yerr=history['logp'][1])\n",
    "axes[0].set_ylabel(r'$\\log p$')\n",
    "\n",
    "axes[1].errorbar(history['t'], history['logq'][0], yerr=history['logq'][1])\n",
    "axes[1].set_ylabel(r'$\\log q$')\n",
    "\n",
    "axes[2].errorbar(history['t'], history['kl_div'][0], yerr=history['kl_div'][1])\n",
    "axes[2].set_ylabel(r'Reverse KL-divergence $D_{\\rm KL}(q || p)$')\n",
    "\n",
    "axes[3].errorbar(history['t'], history['ess'][0], yerr=history['ess'][1])\n",
    "axes[3].set_ylabel('ESS')\n",
    "\n",
    "axes[4].errorbar(history['t'], history['Z'][0], yerr=history['Z'][1])\n",
    "axes[4].set_ylabel('Z')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f0f7d-5476-4f31-8616-991e0de4780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model likelihood\n",
    "logq = logr - logJ\n",
    "print('avg logq =', logq.mean().item())\n",
    "print('std logq =', logq.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8c4c3-7f88-4dba-967b-4f566d6d72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target likelihood\n",
    "logp = -action(U_0) + log_haar_su3(x_0)\n",
    "print('avg logp =', logp.mean().item())\n",
    "print('std logp =', logp.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c549d6-2114-4da6-b8b1-6527860d51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Sample Size\n",
    "print('ESS =', compute_ess(logp, logq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea47b9c-fe05-4464-8e0d-b601e0896004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sun_diffusion)",
   "language": "python",
   "name": "sun_diffusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
