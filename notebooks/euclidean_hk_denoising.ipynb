{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6804cb0e-82ee-4773-8982-97b0a0e1b0f7",
   "metadata": {},
   "source": [
    "# Euclidean Heat Kernel - Diffusion & Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a25e01-baf2-43d9-8049-3f991b0e8a28",
   "metadata": {},
   "source": [
    "In this notebook, we will show how we can\n",
    "- use the exact score function for the Euclidean heat kernel to reverse the forward diffusion process\n",
    "- train a neural network to approximate this score function and replicate the same denoising procedure\n",
    "  \n",
    "The idea is that we can have a direct comparison between our ML results and the analytical results, since we know the score function for the heat kernel exactly.\n",
    "\n",
    "*Note: In more practical applications when we start with non-trivial 'training data' (i.e., give the heat equation initial conditions), we do not know the true score function for all time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b379ee-bdc8-4416-a054-7fea6a6b9308",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3bdde-c409-41d4-9b0a-053790543a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04966014-fa29-4081-8dd6-cda9e006c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from our repo\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # repo source code\n",
    "\n",
    "from src.diffusion import VarianceExpandingDiffusion\n",
    "from src.heat import eucl_score_hk\n",
    "from src.utils import grab, wrap\n",
    "from src.devices import set_device, get_device, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7d957-0069-4af7-ab5f-34efa34e60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_device('cuda', 0)\n",
    "print(summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649c97e-5e44-4844-9715-92adb496ef3b",
   "metadata": {},
   "source": [
    "## Background: Euclidean Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc7a54-65e7-4263-b429-b9817a7d9f0e",
   "metadata": {},
   "source": [
    "Variance-expanding diffusion is a stochastic process described by $$dx = g(t) dW$$ that corresponds to the heat equation: $$\\partial_t u(x, t) = \\frac{g(t)^2}{2} \\Delta u(x, t),$$ where $g^2$ is a postive, time-dependent scalar quantity we call the *diffusivity*. The differential operator $\\Delta$ is the Laplace-Beltrami operator on whatever ambient space the samples $x$ occupy. In Euclidean space, $x \\in \\mathbb{R}^n$, $\\Delta$ is just the familiar Laplacian given by $\\Delta := \\sum_{i=1}^n \\partial_i^2$. In particuar, for a single, real-valued degree of freedom (with constant unit diffusivity), the heat equation is $$\\partial_t u = \\partial_x^2 u,$$ and the fundamental solution (Green's function / propagator) for this PDE is called the *Heat Kernel* and is given by $$K(x, t) = \\frac{1}{\\sqrt{2\\pi\\sigma(t)^2}}e^{-\\frac{x^2}{2\\sigma(t)^2}},$$ where $\\sigma(t)$ is the marginal standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630983d-bc27-4ddb-b44b-9d9285f37b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_heat_kernel(x, t, width=None):\n",
    "    \"\"\"Computes the Euclidean heat kernel density K(x, t) for `x` at time `t`.\"\"\"\n",
    "    if width is None:\n",
    "        width = t**0.5  # assume unit constant diffusivity\n",
    "    normalization = 1 / (2 * np.pi * width**2)**0.5\n",
    "    weight = torch.exp(-x**2 / (2 * width**2))\n",
    "    return normalization * weight\n",
    "\n",
    "\n",
    "def _visualize_heat_kernel():\n",
    "    xs = torch.linspace(-4, 4, 100)\n",
    "    times = np.linspace(0.05, 1, 20)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('Heat Kernel Density')\n",
    "    for t in times:\n",
    "        line = ax.plot(grab(xs), grab(euclidean_heat_kernel(xs, t)), color=cmap(t))#, label=f'$t = {t}$')\n",
    "    time_colors = mpl.cm.ScalarMappable(mpl.colors.Normalize(times[0], times[-1]))\n",
    "    cbar = fig.colorbar(time_colors, cmap=cmap, ax=ax, label='$t$')\n",
    "    fig.show()\n",
    "\n",
    "_visualize_heat_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed2f18-973d-43a2-a4a0-859479898a3f",
   "metadata": {},
   "source": [
    "One can easily sample from the Euclidean heat kernel at arbitrary time $t$; since $K(x, t)$ is always a normal distribution, you only need to know the marginal standard deviation at $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e11e2-9146-4675-b6a3-6d693ae6c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_euclid_hk(batch_size, t, width=None):\n",
    "    \"\"\"Generates `batch_size` samples from the Euclidean\n",
    "    Heat kernel at time `t` with width `width`.\"\"\"\n",
    "    if width is None:\n",
    "        width = t**0.5\n",
    "    x_t = width * torch.randn((batch_size,))\n",
    "    return x_t\n",
    "\n",
    "def _test_sample_hk():\n",
    "    batch_size = 16384\n",
    "    xs = torch.linspace(-4, 4, 100)\n",
    "    times = [0.1, 0.5, 1.0]\n",
    "    \n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('Density')\n",
    "    for t in times:\n",
    "        xt = sample_euclid_hk(batch_size, t)\n",
    "        ax.hist(grab(xt), bins=50, density=True, color=cmap(t), alpha=0.55, label=f'Heat kernel samples at $t = {t}$')\n",
    "        ax.plot(grab(xs), grab(euclidean_heat_kernel(xs, t)), ls='--', color=cmap(t), label=f'Analytical HK at $t = {t}$')\n",
    "    fig.legend(loc='right', fontsize='small')\n",
    "    ax.set_title('Heat Kernel Density')\n",
    "    fig.show()\n",
    "\n",
    "_test_sample_hk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145a8cd-f21e-4d88-a2e2-4dd7f817a5e5",
   "metadata": {},
   "source": [
    "It's also easy to simulate the forward process by directly feeding initial data (zero) into our diffuser object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e0e3f-433c-4b06-977b-412ee7e6202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_euclid_fwd():\n",
    "    \"\"\"Simulates forward Euclidean VE diffusion process.\"\"\"\n",
    "    batch_size = 4096\n",
    "    x_0 = torch.zeros((batch_size, 1))\n",
    "    diffuser = VarianceExpandingDiffusion(sigma=1.1)\n",
    "    \n",
    "    xs = torch.linspace(-4, 4, 100)\n",
    "    times = [0.01, 0.1, 0.5, 1.0]\n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "    fig.suptitle('Density During Forward Process')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    for t, ax in zip(times, axes):\n",
    "        # Samples stats should match params of analytical HK\n",
    "        x_t = diffuser(x_0, torch.tensor(t).repeat(batch_size))\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        assert torch.allclose(torch.std(x_t), sigma_t, atol=5e-2), \\\n",
    "            f'StDev of samples {torch.std(x_t).item():.4f} does not match marginal HK StDev {sigma_t:.4f}'\n",
    "        \n",
    "        # Visualize forward process over time\n",
    "        ax.set_title(f'$t = {t}$')\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.hist(grab(x_t), bins=50, density=True, color=cmap(t), alpha=0.55, label=f'Diffused samples at $t = {t}$')\n",
    "        ax.plot(grab(xs), grab(euclidean_heat_kernel(xs, t, width=sigma_t)), ls='--', color=cmap(t), label=f'Analytical HK at $t = {t}$')\n",
    "        ax.legend()\n",
    "    fig.show()\n",
    "\n",
    "visualize_euclid_fwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887b5cf-b0c0-4129-9a80-0ebeed22d0b9",
   "metadata": {},
   "source": [
    "## Denoising with the Exact Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be7800-7687-4670-9dff-b2af378ae225",
   "metadata": {},
   "source": [
    "The analytical score function for the Euclidean heat kernel is simply given by:\n",
    "\n",
    "$$s(x, t) := \\partial_x \\log K(x, t) = -\\frac{x}{\\sigma(t)^2}$$\n",
    "\n",
    "which can be used to solve either the ODE or SDE in reverse time: $$d\\tilde{x} = -g(t)^2 s(x, t)dt + g(t) dW$$ $$d\\tilde{x} = -\\frac{1}{2}g(t)^2 s(x, t)dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a701b3c-231b-4526-83e0-07121b24c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_backward(x_1, diffuser, num_steps=200, solver_type='ODE'):\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    x_t = x_1.clone()\n",
    "\n",
    "    trajectories = []\n",
    "    for step in range(num_steps):\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = eucl_score_hk(x_t, width=sigma_t)\n",
    "        \n",
    "        # Integration step\n",
    "        if solver_type == 'ODE':\n",
    "            x_t = x_t + 0.5 * g_t**2 * score * dt\n",
    "        elif solver_type == 'SDE':\n",
    "            x_t = x_t + 0.5*g_t**2 * score * dt + g_t * torch.rand_like(x_t) * dt**0.5  # TODO: debug\n",
    "        else:\n",
    "            raise NotImplementedError(f'Integration method {solver_type} not supported')\n",
    "        t -= dt\n",
    "        trajectories.append(x_t)\n",
    "    return x_t, trajectories\n",
    "\n",
    "\n",
    "def _test_denoise():\n",
    "    batch_size = 8192\n",
    "    x_1 = sample_euclid_hk(batch_size, t=1.0)\n",
    "    diffuser = VarianceExpandingDiffusion(sigma=1.1)\n",
    "    \n",
    "    num_steps = 200\n",
    "    x_0, trajectories = denoise_backward(x_1, diffuser, num_steps)\n",
    "    \n",
    "    cmap = mpl.colormaps.get_cmap('viridis')\n",
    "    xs = torch.linspace(-4, 4, 100)\n",
    "    steps = [20, 50, 150, 195]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(steps), figsize=(4*len(steps), 4), sharey=True)\n",
    "    fig.suptitle('Density During Reverse Process')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    for ax, step in zip(axes, steps):\n",
    "        # Check that stats of denoised samples match params of analytical HK\n",
    "        t = 1 - step/num_steps\n",
    "        x_t = trajectories[step]\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        #assert torch.allclose(torch.std(x_t), sigma_t, atol=5e-2), \\\n",
    "        #    f'StDev of samples {torch.std(x_t).item():.4f} does not match marginal HK StDev {sigma_t:.4f}'\n",
    "        \n",
    "        # Display reverse process plots\n",
    "        ax.set_title(f'$t = {t:.3f}$')\n",
    "        ax.set_xlabel('$x$')\n",
    "        hk = euclidean_heat_kernel(xs, t)\n",
    "        hk /= hk.sum() * (xs[1] - xs[0])\n",
    "        ax.plot(grab(xs), grab(hk), ls='--', color=cmap(t), label=f'Analytic HK')\n",
    "        ax.hist(grab(x_t), bins=50, color=cmap(t), alpha=0.55, density=True, label=f'Denoised samples')\n",
    "        ax.legend(fontsize=7.5)\n",
    "    fig.show()\n",
    "\n",
    "_test_denoise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115fba6-725b-4f59-a283-dc54557452f6",
   "metadata": {},
   "source": [
    "## Denoising with a Trained Score Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45551a30-2ac1-4a10-bbdc-3fa79844f267",
   "metadata": {},
   "source": [
    "First we build a very simple MLP as our score network, which will input both $x$ and $t$ (for now, no special embeddings for $t$, just handle the raw time as-is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2471cbe-9aaf-44a4-8d84-34093d36199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 8),  # data & time = 1 + 1 dims\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 1))\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        if len(t.shape) == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        return self.net(torch.cat([x_t, t], dim=-1))\n",
    "\n",
    "\n",
    "def _test_score_net():\n",
    "    batch_size = 100\n",
    "    x = torch.randn((batch_size, 1))\n",
    "    t = torch.rand((batch_size,))\n",
    "    s_t = ScoreNet()(x, t)\n",
    "    print('x shape:', x.shape)\n",
    "    print('t shape:', t.shape)\n",
    "    print('s_t shape:', s_t.shape)\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        'Score output should have same shape as input'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_score_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82249ff-0864-4acc-b998-98cebeb53980",
   "metadata": {},
   "source": [
    "Define the conventional **score matching** loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902f6b1-b135-4a8e-9398-4770410f9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss(x_0, diffuser, score_net, tol=1e-5):\n",
    "    t = torch.rand((x_0.size(0),))\n",
    "    t = tol + (1 - tol) * t  # stability near endpoints\n",
    "    \n",
    "    # Diffuse x_0 -> x_t, get s(x_t, t)\n",
    "    x_t = diffuser(x_0, t.squeeze())\n",
    "    sigma_t = diffuser.sigma_func(t)[:, None]\n",
    "    score = score_net(x_t, t) / sigma_t  # rescale score net\n",
    "    #NOTE: make sure shapes line up!! Can get silent side-effects\n",
    "    \n",
    "    # s(x_t, t) should approximate grad log N(x_t; x_0, sigma_t^2)\n",
    "    eps = (x_t - x_0) / sigma_t\n",
    "    loss = (sigma_t * score + eps)**2  # weight factor of sigma(t)^2 for stability\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616f317-7e8a-490d-9f41-71930b8fb843",
   "metadata": {},
   "source": [
    "Now do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062f701-dbcf-4aba-8a50-f56b33233d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Diffusion params\n",
    "sigma = 1.1\n",
    "diffuser = VarianceExpandingDiffusion(sigma)\n",
    "score_net = ScoreNet()\n",
    "\n",
    "# Training hyperparams\n",
    "lr = 1e-3\n",
    "epochs = 1000\n",
    "batch_size = 1024\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "x_0 = torch.zeros((batch_size, 1))\n",
    "losses = []\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = score_matching_loss(x_0, diffuser, score_net)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {loss.item():.6f}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea3c52-2c85-4fc8-898b-8f0bcb23fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.plot(losses, lw=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2279a6-063a-41f1-8382-7434cea4887a",
   "metadata": {},
   "source": [
    "To generate new samples, we define a function to sample from the diffusion model posterior using our trained score network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06206ec1-992d-4cf5-8cfa-8afbe6739fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def euler_sampler(x_1, score_net, diffuser, num_steps=200, solver_type='ODE', verbose=False):\n",
    "    score_net.eval()\n",
    "    batch_size = x_1.size(0)\n",
    "    \n",
    "    trajectories = []\n",
    "    dt = 1 / num_steps\n",
    "    x_t = x_1.clone()\n",
    "    t = 1.0\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Get ODE / SDE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t, torch.tensor(t).repeat(batch_size,)) / sigma_t\n",
    "\n",
    "        # Integration step\n",
    "        if solver_type == 'ODE':\n",
    "            x_t = x_t + 0.5 * g_t**2 * score * dt  # ODE Euler step\n",
    "        elif solver_type == 'SDE':\n",
    "            x_t = x_t + g_t**2 * score * dt + g_t * torch.rand_like(x_t) * dt**0.5  # SDE step\n",
    "        else:\n",
    "            raise NotImplementedError(f'Integration method {solver_type} not supported')\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        trajectories.append(x_t)\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps} | x_t = {x_t.mean().item():.6f}')\n",
    "    return x_t, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa98f18-b7e0-4d8e-8e38-e32a1c4b555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data: zeros\n",
    "num_samples = 4096\n",
    "x_0 = torch.zeros((num_samples, 1))\n",
    "\n",
    "# Diffuse forward: x_0 -> x_1\n",
    "sigma = 1.1\n",
    "diffuser = VarianceExpandingDiffusion(sigma)\n",
    "x_1 = diffuser(x_0, t=torch.ones((num_samples,)))\n",
    "\n",
    "# Denoise backward: x_1 -> x_0'\n",
    "num_steps = 100\n",
    "x_0, trajectories = euler_sampler(x_1, score_net, diffuser, num_steps, solver_type='ODE')\n",
    "print('New x_0:', grab(x_0.mean().item()))\n",
    "\n",
    "# Plot trajectories\n",
    "times = [1.0, 0.75, 0.5, 0.25, 0.05]\n",
    "xs = torch.linspace(-5, 5, 100)\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "fig.suptitle('Learned Denoising Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "for t, ax in zip(times, axes):\n",
    "    # Denoised samples\n",
    "    x_t = trajectories[int(num_steps * (1 - t))]\n",
    "    ax.hist(grab(x_t), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "\n",
    "    # Analytical heat kernel\n",
    "    hk = euclidean_heat_kernel(xs, t, width=diffuser.sigma_func(t))\n",
    "    hk /= hk.sum() * (xs[1] - xs[0])\n",
    "    ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytic HK')\n",
    "    ax.set_xlabel(r'$x_t$')\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd470c0a-8ef1-4288-8101-b91e6a08f4b7",
   "metadata": {},
   "source": [
    "### Include time embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cc20e-ee47-4819-92b8-587fef76f38a",
   "metadata": {},
   "source": [
    "What if we use time embedding in our score network? Now we enable the score net to learn a good feature embeddings for the time as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77696a37-942f-4ba2-95e9-4e3f0214c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFourierProjection(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, scale=30.0):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = torch.randn(embed_dim // 2) * scale\n",
    "        self.W = torch.nn.Parameter(weights, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :]\n",
    "\n",
    "        sinx = torch.sin(2*np.pi * x_proj)\n",
    "        cosx = torch.cos(2*np.pi * x_proj)\n",
    "\n",
    "        return torch.cat([sinx, cosx], dim=-1)\n",
    "\n",
    "\n",
    "def _test_embedding():\n",
    "    print('[Testing GaussianFourierProjection...]')\n",
    "    batch_size = 5\n",
    "    t = torch.rand((batch_size,))\n",
    "    embed_dim = 64\n",
    "    embedder = GaussianFourierProjection(embed_dim)\n",
    "    t_embed = embedder(t)\n",
    "    assert t_embed.shape == (batch_size, embed_dim), \\\n",
    "        '[FAILED: embedded times must have shape [batch_size, embed_dim]]'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d972666-ca4e-42e2-80b2-db91b583bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbeddedScoreNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.time_embedder = torch.nn.Sequential(  # embeds times\n",
    "            GaussianFourierProjection(embed_dim),\n",
    "            torch.nn.Linear(embed_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "        )\n",
    "        self.encoder = torch.nn.Sequential(  # encodes data\n",
    "            torch.nn.Linear(1, hidden_dim),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(  # decodes data + times\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, x_t, t):\n",
    "        t_emb = self.time_embedder(t)  # [B] -> [B, E] -> [B, H]\n",
    "        x_enc = self.encoder(x_t)  # [B, 1] -> [B, H]\n",
    "        y = x_enc + t_emb  # additive embedding [B, H]\n",
    "        return self.decoder(y)\n",
    "\n",
    "\n",
    "def _test_score_net():\n",
    "    batch_size = 100\n",
    "    x = torch.randn((batch_size, 1))\n",
    "    t = torch.rand((batch_size,))\n",
    "    \n",
    "    s_t = TimeEmbeddedScoreNet(hidden_dim=8, embed_dim=64)(x, t)\n",
    "    print('x shape:', x.shape)\n",
    "    print('t shape:', t.shape)\n",
    "    print('s_t shape:', s_t.shape)\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        'Score output should have same shape as input'\n",
    "    print('[PASSED]')\n",
    "\n",
    "\n",
    "_test_score_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4bc7f-7c2d-424d-a861-0e8e2233b7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Diffusion params\n",
    "sigma = 1.1\n",
    "diffuser = VarianceExpandingDiffusion(sigma)\n",
    "\n",
    "# Score network\n",
    "hidden_dim = 8\n",
    "embed_dim = 256\n",
    "score_net = TimeEmbeddedScoreNet(hidden_dim, embed_dim)\n",
    "\n",
    "# Training hyperparams\n",
    "lr = 1e-3\n",
    "epochs = 1000\n",
    "batch_size = 1024\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "x_0 = torch.zeros((batch_size, 1))\n",
    "losses = []\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = score_matching_loss(x_0, diffuser, score_net)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {loss.item():.6f}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83edbf-53f3-428f-8c68-e020c5fe6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.plot(losses, lw=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211e3ee-3863-4c5a-97f2-6a234896b600",
   "metadata": {},
   "source": [
    "So the loss curve definitely has a different shape, but doesn't quite seem to converge to as low of a value..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e499522-e2fd-4fd2-92a6-ec0cd8747a50",
   "metadata": {},
   "source": [
    "Let's generate new samples again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf9880-27e7-4298-9560-14ca757d4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data: zeros\n",
    "num_samples = 4096\n",
    "x_0 = torch.zeros((num_samples, 1))\n",
    "\n",
    "# Diffuse forward: x_0 -> x_1\n",
    "sigma = 1.1\n",
    "diffuser = VarianceExpandingDiffusion(sigma)\n",
    "x_1 = diffuser(x_0, t=torch.ones((num_samples,)))\n",
    "\n",
    "# Denoise backward: x_1 -> x_0'\n",
    "num_steps = 100\n",
    "x_0, trajectories = euler_sampler(x_1, score_net, diffuser, num_steps, solver_type='ODE')\n",
    "print('New x_0:', grab(x_0.mean().item()))\n",
    "\n",
    "# Plot trajectories\n",
    "times = [1.0, 0.75, 0.5, 0.25, 0.05]\n",
    "xs = torch.linspace(-5, 5, 100)\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "fig.suptitle('Learned Denoising Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "for t, ax in zip(times, axes):\n",
    "    # Denoised samples\n",
    "    x_t = trajectories[int(num_steps * (1 - t))]\n",
    "    ax.hist(grab(x_t), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "\n",
    "    # Analytical heat kernel\n",
    "    hk = euclidean_heat_kernel(xs, t, width=diffuser.sigma_func(t))\n",
    "    hk /= hk.sum() * (xs[1] - xs[0])\n",
    "    ax.plot(grab(xs), grab(hk), color=cmap(t), ls='--', label='Analytic HK')\n",
    "    ax.set_xlabel(r'$x_t$')\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17728a4-7758-4e75-88c4-5de309cb4c2e",
   "metadata": {},
   "source": [
    "Seems like the performance is, at best, no better than without time embedding. Maybe this example is simple enough that just inputting raw times is sufficiently expressive to recover the heat kernel on a single d.o.f?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54538708-80cd-4800-90a8-268f54352942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
