{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d6ca7-f107-4624-a0de-e19baf4e67ca",
   "metadata": {},
   "source": [
    "# Spectral Diffusion for a Toy ${\\rm SU}(2)$ Model using Group-Valued Score Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea60c33-70af-4bf1-b975-e186784033a1",
   "metadata": {},
   "source": [
    "In this notebook, we apply the same machinery we used in the other notebook (where we trained a score network to learn the heat kernel and denoise the variance-expanding diffusion process) to learn a toy theory involving a single ${\\rm SU}(2)$ matrix $U$ (one independent eigenangle $\\theta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6096b-1be2-4312-b2fe-52a4b3fe25ce",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a312c-db55-4997-a9cd-9ab97c8c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c8056-2c0b-40dc-8076-40fad7bc5cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sun_diffusion\n",
    "import sun_diffusion.analysis as al  # analysis library\n",
    "\n",
    "from sun_diffusion.action import SUNToyPolynomialAction\n",
    "from sun_diffusion.sun import (\n",
    "    mat_angle, adjoint,\n",
    "    extract_diag, embed_diag,\n",
    "    random_sun_element, random_un_haar_element,\n",
    ")\n",
    "from sun_diffusion.diffusion import VarianceExpandingDiffusionSUN, PowerDiffusionSUN\n",
    "from sun_diffusion.heat import sun_score_hk\n",
    "from sun_diffusion.utils import grab, compute_ess, compute_kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbeaf2-083f-49fb-ace6-2ed14d37d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sun_diffusion.devices import get_device, get_dtype, set_device, summary\n",
    "set_device('cuda', 0)\n",
    "print(summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c31ee1-6700-4dc7-aa2d-2e9b6918e69f",
   "metadata": {},
   "source": [
    "## Define a Target Theory and Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0531e4-270f-4264-b811-0c15c90751d9",
   "metadata": {},
   "source": [
    "The target theory whose distribution we will try to reproduce is specified by a toy action of our choosing, which we define to be \n",
    "$$S_i(U) = -\\frac{\\beta}{2} {\\rm Re}{\\rm Tr}\\left[\\sum_n c_n U^n\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223b0c6-14c7-4913-9d71-72565d689fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 2\n",
    "beta = 1.0\n",
    "coeffs_dict = {\n",
    "    0: [1.0, 0., 0.],\n",
    "    1: [0.17, -0.65, 1.22],\n",
    "    2: [0.98, -0.63, -0.21]\n",
    "}  # from Table I of [2008.05456]\n",
    "\n",
    "coeffs = 1  # change this to try other coefficient sets\n",
    "action = SUNToyPolynomialAction(beta, coeffs_dict[coeffs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058173a7-080b-45f9-a955-df152e9fa135",
   "metadata": {},
   "source": [
    "To generate configurations, we will use the Metropolis algorithm for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02599f4b-8d5a-401f-a798-98e0f341a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_metropolis(batch_size, Nc, action, num_therm, num_iters, step_size, save_freq=10):\n",
    "    \"\"\"Batched Metropolis sampler.\"\"\"\n",
    "    action_vals = []\n",
    "    accept_rates = []\n",
    "    \n",
    "    U = random_sun_element(batch_size, Nc=Nc)\n",
    "    ens = []\n",
    "    for i in tqdm.tqdm(range(-num_therm, num_iters)):\n",
    "        # Proposal\n",
    "        V = random_sun_element(batch_size, Nc=Nc, scale=step_size)\n",
    "        Up = V @ U\n",
    "        dS = action(Up) - action(U)\n",
    "\n",
    "        # Accept / Reject\n",
    "        r = torch.rand(batch_size)  # accept w/ prob = exp(-dS)\n",
    "        accept_mask = (r < torch.exp(-dS))[:, None, None]\n",
    "        U = torch.where(accept_mask, Up, U)\n",
    "\n",
    "        action_vals.append(grab(action(U).mean()))\n",
    "        accept_rates.append(grab(torch.sum(accept_mask) / batch_size))\n",
    "        if i >= 0 and (i+1) % save_freq == 0:\n",
    "            ens.append(U)\n",
    "    return torch.cat(ens), action_vals, accept_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef48d5-b65c-43e0-b6be-f4455ee1c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "batch_size = 32\n",
    "num_therm = 1_000\n",
    "num_iters = 10_240\n",
    "save_freq = 10\n",
    "step_size = 0.9\n",
    "\n",
    "num_train = batch_size * (num_iters // save_freq)\n",
    "print(f'{num_train=}')\n",
    "\n",
    "U_train, action_vals, accept_rates = apply_metropolis(\n",
    "    batch_size = batch_size,\n",
    "    Nc = Nc,\n",
    "    action = action,\n",
    "    num_therm = num_therm,\n",
    "    num_iters = num_iters,\n",
    "    step_size = step_size\n",
    ")\n",
    "print('U_train shape:', U_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30358ff4-d9c5-4e3f-a7fb-eaa2ae243a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Metropolis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Metropolis Steps')\n",
    "\n",
    "axes[0].plot(*al.bin_data(action_vals, binsize=100))\n",
    "axes[0].set_ylabel('Average Action')\n",
    "\n",
    "axes[1].plot(*al.bin_data(accept_rates, binsize=100))\n",
    "axes[1].set_ylabel('Acceptance Rate')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1606b-c4d3-4468-8724-cd0a3c9c3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(grab(mat_angle(U_train[::10])[0][:, 0]), bins=50, density=True, label='MCMC Samples')\n",
    "ths = torch.linspace(-np.pi, np.pi, steps=101)\n",
    "dth = ths[1] - ths[0]\n",
    "ths = torch.stack([ths, -ths], dim=-1)\n",
    "ps = torch.exp(-action.value_eigs(ths)) * torch.sin(ths[:, 0])**2\n",
    "ps /= torch.sum(ps * dth, dim=-1, keepdims=True)\n",
    "print('ps shape:', ps.shape)\n",
    "ax.plot(grab(ths[:, 0]), grab(ps), color='k', linestyle='--', label='Target')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6f5fa-0380-4201-9c6c-3d9596a8d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = 3.0\n",
    "# diffuser = VarianceExpandingDiffusionSUN(kappa)\n",
    "diffuser = PowerDiffusionSUN(kappa, alpha=1)\n",
    "\n",
    "times = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(3*len(times), 3), sharey=True)\n",
    "axes[0].set_ylabel(r'$p_t(\\theta)$', fontsize=14)\n",
    "\n",
    "bins = np.linspace(-np.pi, np.pi, num=51)\n",
    "\n",
    "U_0 = U_train.clone()[::10]\n",
    "for t, ax in zip(times, axes):\n",
    "    if t == 0:  # avoid sampling from HK at t=0\n",
    "        x_t, _, _ = mat_angle(U_0)\n",
    "    else:\n",
    "        U_t, _, _ = diffuser.diffuse(U_0, t*torch.ones(U_0.size(0)), n_iter=15)\n",
    "        x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=bins, density=True, color=cmap(t), alpha=0.65)\n",
    "    ax.set_xlabel(r'$\\theta_t$', fontsize=14)\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xticks([-np.pi, 0, np.pi])\n",
    "    ax.set_xticklabels([r\"$-\\pi$\", r\"$0$\", r\"$\\pi$\"])\n",
    "    ax.xaxis.set_minor_locator(mpl.ticker.FixedLocator([-np.pi/2, np.pi/2]))\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# also save histograms for future comparisons\n",
    "true_times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.10, 0.05, 0.01]\n",
    "true_hists = []\n",
    "for t in tqdm.tqdm(true_times):\n",
    "    hist_xt = np.zeros(len(bins)-1, dtype=np.float64)\n",
    "    for chunk_U in torch.chunk(U_0, 10):\n",
    "        U_t, _, _ = diffuser.diffuse(chunk_U, t*torch.ones(chunk_U.size(0)), n_iter=15)\n",
    "        x_t, _, _ = mat_angle(U_t)\n",
    "        hist_xt += np.histogram(grab(x_t[:, 0]), bins=bins)[0]\n",
    "    hist_xt /= U_0.size(0) * (bins[1]-bins[0])\n",
    "    true_hists.append((bins, hist_xt))\n",
    "\n",
    "# Target density\n",
    "ths = torch.linspace(-np.pi, np.pi, steps=101)\n",
    "dth = grab(ths[1]-ths[0])\n",
    "ths = torch.stack([ths, -ths], dim=-1)\n",
    "ps = grab((-action.value_eigs(ths)).exp()) * np.sin(grab(ths[:,0]))**2\n",
    "ps /= np.sum(ps, axis=-1, keepdims=True) * dth\n",
    "target_line, = axes[0].step(grab(ths[:, 0]), ps, color='k', label='Target')\n",
    "\n",
    "# Haar uniform\n",
    "xs = torch.linspace(-np.pi, np.pi, 100)\n",
    "haar = (1 / np.pi) * torch.sin(xs)**2\n",
    "haar_line, = axes[-1].plot(grab(xs), grab(haar), ls='--', color='red', label='Haar Uniform')\n",
    "\n",
    "sample_patch = mpl.patches.Patch(facecolor='gray', alpha=0.4, label='Diffused samples')\n",
    "axes[-1].legend(\n",
    "    handles=[target_line, haar_line, sample_patch],\n",
    "    loc='upper right',\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc9f7c-2707-479f-bed3-39ee349f08f5",
   "metadata": {},
   "source": [
    "## Train a Score Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ff214-9f53-42d8-ac52-5c71797e053a",
   "metadata": {},
   "source": [
    "Now we must construct a score network that will take as input the eigenangle $\\theta$ and time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bb47e-3908-424a-b908-000451af94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SU2ScoreNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        assert input_dim % 2 == 1\n",
    "        self.nk = (input_dim - 1)//2\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # input_dim = 1 eigenangle + encoded time\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x_t, t):\n",
    "        assert len(x_t.shape) == 2, \\\n",
    "            'input eigenangles shape should be [batch_size, Nc-1]'\n",
    "        assert len(t.shape) == 1, \\\n",
    "            'times should only have a batch dimension'\n",
    "        cos_t = torch.cos(torch.exp(-10.0*(torch.arange(1,self.nk+1)/self.nk - 0.5))*t.unsqueeze(-1))\n",
    "        sin_t = torch.sin(torch.exp(-10.0*(torch.arange(1,self.nk+1)/self.nk - 0.5))*t.unsqueeze(-1))\n",
    "        inp = torch.cat([x_t, cos_t, sin_t], dim=-1)\n",
    "        return self.net(inp) * torch.sin(x_t) # enforce score -> 0 at endpoints\n",
    "\n",
    "\n",
    "def _test_su2_score_net():\n",
    "    batch_size = 10\n",
    "    Nc = 2\n",
    "    x = 2*np.pi*torch.rand((batch_size, 1)) - np.pi\n",
    "    t = torch.rand((batch_size,))\n",
    "    s_t = SU2ScoreNet()(x, t)\n",
    "\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        '[FAILED: score net output must have same shape as input data]'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_su2_score_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4343631-6c53-4658-af5b-b290dfaa8273",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11c5d4-00fd-4d13-9cf2-e5484542d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss_sun(U_0, diffuser, score_net, tol=1e-4):\n",
    "    batch_size = U_0.size(0)\n",
    "    t = tol + (1 - tol) * torch.rand(batch_size)\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "\n",
    "    U_t, xs, V = diffuser.diffuse(U_0, t, n_iter=25)\n",
    "    x_t, P, Pinv = mat_angle(U_t)\n",
    "\n",
    "    score = score_net(x_t[..., :-1], t)\n",
    "    # NOTE(gkanwar): Passing xs instead of (x_t - x_0) is important here\n",
    "    true_score_xs = sun_score_hk(xs[..., :-1], width=sigma_t)\n",
    "    true_score = extract_diag(Pinv @ V @ embed_diag(true_score_xs).to(V) @ adjoint(V) @ P).real[...,:-1]\n",
    "\n",
    "    # NOTE(gkanwar): Scaling by sigma_t^2 results in a roughly constant variance,\n",
    "    # making training much more stable.\n",
    "    diff = score - true_score\n",
    "    loss = torch.mean(sigma_t[:, None]**2 * diff**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f0c2-eff4-4088-ab04-8a86904a35f4",
   "metadata": {},
   "source": [
    "Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90fe27-8301-4a46-8b01-6d9871b52b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Make diffusion process\n",
    "kappa = 3.0\n",
    "alpha = 1.0\n",
    "diffuser = PowerDiffusionSUN(kappa, alpha)\n",
    "#diffuser = VarianceExpandingDiffusionSUN(kappa)\n",
    "\n",
    "score_net = SU2ScoreNet(input_dim=51, hidden_dim=64)\n",
    "\n",
    "# Setup training hyperparams\n",
    "epochs = 200\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Prepare dataloader\n",
    "batch_size = 1024\n",
    "dataset = TensorDataset(U_train) \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=get_device()))\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "score_net.train()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (U_0_batch,) in enumerate(tqdm.tqdm(dataloader, leave=False)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = score_matching_loss_sun(U_0_batch, diffuser, score_net)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += grab(loss)\n",
    "        losses.append(grab(loss))\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb9d44-9413-4741-8eca-f93fcb4923cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss vs epochs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2.5))\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "xs, ys = al.bin_data(losses, binsize=25)\n",
    "xs = xs / (num_train/batch_size) # convert to epochs\n",
    "ax.plot(xs, ys, lw=0.75)\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99beaf-1386-49b6-af23-b8f8afa33b88",
   "metadata": {},
   "source": [
    "Compare the learned score function to the force for the target theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c6405-5491-444d-9882-16863a618061",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_eval = torch.linspace(-np.pi, np.pi, 200)\n",
    "\n",
    "learned_score = score_net(xs_eval.unsqueeze(-1), torch.zeros(len(xs_eval))) \n",
    "learned_score /= learned_score.abs().max()\n",
    "\n",
    "true_score = action.force_eigs(xs_eval)\n",
    "true_score /= true_score.abs().max()\n",
    "\n",
    "plt.plot(grab(xs_eval), grab(learned_score), label='Learned Score')\n",
    "plt.plot(grab(xs_eval), grab(true_score), '--', label='True Score')\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Force / Score')\n",
    "plt.title('Learned Score & Force at $t=0$')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e527f41-80d1-498f-8949-9f5c40098ed4",
   "metadata": {},
   "source": [
    "## Denoising Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b92cd-adab-4a8a-a98e-52827de40b77",
   "metadata": {},
   "source": [
    "Now we run the reverse process to generate new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f004c1-b6c1-49eb-b243-87a17602e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sun_gaussian(shape):\n",
    "    Nc, Nc_ = shape[-2:]\n",
    "    assert Nc == Nc_\n",
    "    return proj_to_algebra(torch.randn(shape) + 1j*torch.randn(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad594ece-104e-4fe5-ad26-6046fd58610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_reverse_ODE_eigs(U_1, logr, score_net, diffuser, num_steps=200, verbose=False):\n",
    "    trajectories = {\n",
    "        'U_t': [],\n",
    "        'logp': [],\n",
    "        'logq': [],\n",
    "        'kl_div': [],\n",
    "        'ess': [],\n",
    "        't': [],\n",
    "        'Z': [],\n",
    "    }\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    batch_size = U_1.size(0)\n",
    "    x_1, V, V_inv = mat_angle(U_1)\n",
    "    x_t = x_1.clone()\n",
    "\n",
    "    logJ = 0.\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Get ODE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((batch_size,)))\n",
    "        jac = torch.func.vmap(torch.func.jacfwd(lambda x: score_net(x[None], torch.tensor(t)[None])[0]))(x_t[..., :-1])\n",
    "        div = torch.einsum('...ii->...', jac)\n",
    "        \n",
    "        # Integration step in reverse time\n",
    "        score = torch.cat([score, -score.sum(-1, keepdim=True)], dim=-1)\n",
    "        x_t = x_t + 0.5 * g_t**2 * score * dt\n",
    "        logJ = logJ + 0.5 * g_t**2 * div * dt\n",
    "\n",
    "        # Eigen-recomposition\n",
    "        D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "        U_t = V @ D @ V_inv\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        logp = -action(U_t) + log_haar_su2(x_t)\n",
    "        logq = logr - logJ\n",
    "        Z = al.bootstrap(grab((logp - logq).exp()), Nboot=1000, f=al.rmean)\n",
    "        kl_div = al.bootstrap(grab(logp), grab(logq), Nboot=1000, f=compute_kl_div)\n",
    "        ess = al.bootstrap(grab(logp), grab(logq), Nboot=1000, f=compute_ess)\n",
    "        if verbose and step % 20 == 0:\n",
    "            print(f'Step {step}/{num_steps}')\n",
    "            print('logp =', logp.mean().item())\n",
    "            print('logq =', logq.mean().item())\n",
    "            print('Dkl =', kl_div)\n",
    "            print('ESS =', ess)\n",
    "            print()\n",
    "        trajectories['t'].append(t)\n",
    "        trajectories['U_t'].append(grab(U_t))\n",
    "        trajectories['logp'].append(al.bootstrap(grab(logp), Nboot=1000, f=al.rmean))\n",
    "        trajectories['logq'].append(al.bootstrap(grab(logq), Nboot=1000, f=al.rmean))\n",
    "        trajectories['kl_div'].append(kl_div)\n",
    "        trajectories['ess'].append(ess)\n",
    "        trajectories['Z'].append(Z)\n",
    "\n",
    "    for key in ['Z', 'kl_div', 'ess', 'logp', 'logq']:\n",
    "        trajectories[key] = np.stack(trajectories[key], axis=1)\n",
    "    \n",
    "    return U_t, logJ, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3119b-6d7e-4e02-ab2c-98268d5992f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_haar_su2(x):\n",
    "    \"\"\"Computes log likelihood of SU(2) Haar uniform density.\"\"\"\n",
    "    x = x[:, 0]\n",
    "    log_sin2 = 2*torch.log(torch.abs(torch.sin(x)))\n",
    "    log_norm = math.log(np.pi)\n",
    "    return log_sin2 - log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca990509-539a-411d-a064-988ef9e6613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_samples = batch_size\n",
    "num_samples = 4096\n",
    "Nc = 2\n",
    "U_1 = random_un_haar_element(num_samples, Nc=Nc)\n",
    "U_1 *= (torch.linalg.det(U_1)**(-1/Nc) * torch.exp(2j*np.pi*torch.randint(Nc, size=(num_samples,))/Nc))[...,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d656e4-edce-4e72-954a-3e627bd65980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get prior log likelihood\n",
    "x_1, _, _ = mat_angle(U_1)\n",
    "logr = log_haar_su2(x_1)\n",
    "print('avg logr =', logr.mean().item())\n",
    "print('std logr =', logr.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74634ca8-29d6-422d-b8b4-00a528e001b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U_0, logJ, history = solve_reverse_ODE_eigs(U_1, logr, score_net, diffuser, num_steps=200, verbose=True)\n",
    "x_0, _, _ = mat_angle(U_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecdb50-52d9-40aa-8f1d-7339145b3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reverse trajectories\n",
    "# times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.01]\n",
    "times = true_times\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(3*len(times), 4), sharey=True)\n",
    "fig.suptitle('SU(2) Angular Spectral Density During Reverse Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "for t, hist, ax in zip(times, true_hists, axes):\n",
    "    # Histogram denoised samples\n",
    "    step = int((1 - t) * len(history['U_t']))\n",
    "    U_t = torch.tensor(history['U_t'][step])\n",
    "    x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "    \n",
    "    # Forward diffused samples\n",
    "    ax.stairs(hist[1], hist[0], ec='red', color='none', label='Target Samples')\n",
    "    \n",
    "    # integrate score to get model distribution\n",
    "    sigma_t = max(0.1, diffuser.sigma_func(t))\n",
    "    bx = score_net(xs, t*torch.ones((xs.size(0),)))[...,0] / sigma_t**2\n",
    "    logp_hat = torch.cumsum(bx, dim=-1) * (xs[1,0]-xs[0,0])\n",
    "    p = grab((logp_hat + log_haar_su2(xs)).exp())\n",
    "    p /= np.sum(p)*grab(xs[1,0]-xs[0,0])\n",
    "\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86755fe-389f-4654-b9fd-fc059c879095",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "t = times[-1]\n",
    "hist = true_hists[-1]\n",
    "\n",
    "# Histogram denoised samples\n",
    "step = int((1 - t) * len(history['U_t']))\n",
    "U_t = torch.tensor(history['U_t'][step])\n",
    "x_t, _, _ = mat_angle(U_t)\n",
    "\n",
    "ax.hist(grab(x_t[:, 0]), bins=50, density=True, color='blue', alpha=0.5, label='Denoised samples')\n",
    "ax.stairs(hist[1], hist[0], ec='red', color='none', label='Target Samples')\n",
    "\n",
    "# Analytical target density\n",
    "ths = torch.linspace(-np.pi, np.pi, steps=101)\n",
    "dth = grab(ths[1]-ths[0])\n",
    "ths_stack = torch.stack([ths, -ths], dim=-1)\n",
    "ps = grab((-action.value_eigs(ths_stack)).exp()) * np.sin(grab(ths))**2\n",
    "ps /= np.sum(ps, axis=-1, keepdims=True) * dth\n",
    "ax.plot(grab(ths), ps, color='black', lw=2, label='Target density')\n",
    "\n",
    "# Format\n",
    "ax.set_xticks([-np.pi, 0, np.pi])\n",
    "ax.set_xticklabels([r\"$-\\pi$\", r\"$0$\", r\"$\\pi$\"], fontsize=12)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Plot\n",
    "ax.set_title(rf'Final Reverse Step ($t={t}$)', fontsize=14)\n",
    "ax.set_ylabel('Density', fontsize=14)\n",
    "ax.set_xlabel(r'$\\theta$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bd3ac-9b75-4601-8520-dcc29689d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _measure_Z():\n",
    "    th = torch.linspace(-np.pi, np.pi, steps=501)\n",
    "    x = torch.stack([th, -th], axis=-1)\n",
    "    U = embed_diag((1j*x).exp())\n",
    "    est_Z = grab((th[1]-th[0])*(-action(U) + log_haar_su2(x)).exp().sum())\n",
    "    return est_Z\n",
    "true_Z = _measure_Z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653af20-b4b2-42d5-8a79-e5eecbdea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(16, 4))\n",
    "\n",
    "axes[0].errorbar(history['t'], history['logp'][0], yerr=history['logp'][1])\n",
    "axes[0].set_ylabel(r'$\\log p$')\n",
    "\n",
    "axes[1].errorbar(history['t'], history['logq'][0], yerr=history['logq'][1])\n",
    "axes[1].set_ylabel(r'$\\log q$')\n",
    "\n",
    "axes[2].errorbar(history['t'], history['kl_div'][0], yerr=history['kl_div'][1])\n",
    "axes[2].set_ylabel(r'Reverse KL-divergence $D_{\\rm KL}(q || p)$')\n",
    "\n",
    "axes[3].errorbar(history['t'], history['ess'][0], yerr=history['ess'][1])\n",
    "axes[3].set_ylabel('ESS')\n",
    "\n",
    "axes[4].errorbar(history['t'], history['Z'][0], yerr=history['Z'][1])\n",
    "axes[4].axhline(true_Z, color='k', linestyle='--')\n",
    "axes[4].set_ylabel('Z')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f0f7d-5476-4f31-8616-991e0de4780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model likelihood\n",
    "logq = logr - logJ\n",
    "print('avg logq =', logq.mean().item())\n",
    "print('std logq =', logq.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8c4c3-7f88-4dba-967b-4f566d6d72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target likelihood\n",
    "logp = -action(U_0) + log_haar_su2(x_0)\n",
    "print('avg logp =', logp.mean().item())\n",
    "print('std logp =', logp.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c549d6-2114-4da6-b8b1-6527860d51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Sample Size\n",
    "print('ESS =', compute_ess(logp, logq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea47b9c-fe05-4464-8e0d-b601e0896004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sun_diffusion)",
   "language": "python",
   "name": "sun_diffusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
