{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d6ca7-f107-4624-a0de-e19baf4e67ca",
   "metadata": {},
   "source": [
    "# Spectral Diffusion for a Toy ${\\rm SU}(2)$ Model using Group-Valued Score Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea60c33-70af-4bf1-b975-e186784033a1",
   "metadata": {},
   "source": [
    "In this notebook, we apply the same machinery we used in the other notebook (where we trained a score network to learn the heat kernel and denoise the variance-expanding diffusion process) to learn a toy theory involving a single ${\\rm SU}(2)$ matrix $U$ (one independent eigenangle $\\theta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6096b-1be2-4312-b2fe-52a4b3fe25ce",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a312c-db55-4997-a9cd-9ab97c8c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cab070-32da-4a8f-8d64-694fd1c4b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from our repo\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # repo source code\n",
    "import analysis as al\n",
    "\n",
    "from src.linalg import trace, adjoint\n",
    "from src.diffusion import VarianceExpandingDiffusion, VarianceExpandingDiffusionSUN, PowerDiffusionSUN\n",
    "from src.sun import (\n",
    "    proj_to_algebra, matrix_exp, matrix_log,\n",
    "    random_sun_element, random_un_haar_element,\n",
    "    group_to_coeffs, coeffs_to_group,\n",
    "    extract_sun_algebra, embed_diag, mat_angle, extract_diag\n",
    ")\n",
    "from src.canon import canonicalize_sun\n",
    "from src.heat import (\n",
    "    #eucl_score_hk,\n",
    "    log_sun_hk, sun_hk,\n",
    "    sun_score_hk_old, sun_score_hk, \n",
    "    sample_sun_hk\n",
    ")\n",
    "from src.utils import grab, wrap\n",
    "from src.devices import set_device, get_device, summary\n",
    "from src.integrate import estimate_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0cda9-0a85-40ef-8d6a-8ed85e23870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import importlib\n",
    "importlib.reload(src.heat)\n",
    "importlib.reload(src.diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbeaf2-083f-49fb-ace6-2ed14d37d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_device('cpu', 0)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c31ee1-6700-4dc7-aa2d-2e9b6918e69f",
   "metadata": {},
   "source": [
    "## Define a Target Theory and Generate Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0531e4-270f-4264-b811-0c15c90751d9",
   "metadata": {},
   "source": [
    "The target theory whose distribution we will try to reproduce is specified by a toy action of our choosing, which we define to be \n",
    "$$S[U] = -\\beta \\; {\\rm Re}{\\rm Tr}(U).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bede832-0017-47bf-a38f-112dd09f8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.action import SUNToyAction\n",
    "\n",
    "# Instantiate and test a toy SU(2) matrix action\n",
    "def _test_action():\n",
    "    batch_size = 3\n",
    "    Nc = 2\n",
    "    U = random_sun_element(batch_size, Nc=2)\n",
    "\n",
    "    action = SUNToyAction(beta=1.0)\n",
    "    print('Action evaluated on configs:', grab(action(U)))\n",
    "\n",
    "_test_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a698bb-c28e-4342-89b2-68d096ccd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SUNToyPolynomialAction:\n",
    "    def __init__(self, beta, coeffs=[1.0, 0.0, 0.0]):\n",
    "        self.beta = beta\n",
    "        self.coeffs = coeffs\n",
    "\n",
    "    def __call__(self, U):\n",
    "        Nc = U.size(-1)\n",
    "        action_density = 0\n",
    "        for i, c in enumerate(self.coeffs):\n",
    "            action_density += c * torch.matrix_power(U, i+1)\n",
    "        return -self.beta * trace(action_density).real / Nc\n",
    "\n",
    "    def value_eigs(self, th):\n",
    "        Nc = th.size(-1)\n",
    "        S = 0\n",
    "        for i, c in enumerate(self.coeffs):\n",
    "            S += c * torch.cos((i+1)*th).sum(-1) / Nc\n",
    "        return -self.beta * S\n",
    "\n",
    "\n",
    "def _test_toy_polynomial_action():\n",
    "    batch_size = 5\n",
    "    Nc = 2\n",
    "    U = random_sun_element(batch_size, Nc=2)\n",
    "    ths, _, _ = mat_angle(U)\n",
    "\n",
    "    beta = 1.0\n",
    "    coeffs = [1.0, 1.0, 1.0]\n",
    "    action = SUNToyPolynomialAction(beta, coeffs)\n",
    "\n",
    "    S = action(U)\n",
    "    S2 = action.value_eigs(ths)\n",
    "    assert torch.allclose(S, S2)\n",
    "    print('Action evaluated on cfgs:', grab(S), grab(S2))\n",
    "\n",
    "_test_toy_polynomial_action()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058173a7-080b-45f9-a955-df152e9fa135",
   "metadata": {},
   "source": [
    "To generate configurations, we will use the Metropolis algorithm for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02599f4b-8d5a-401f-a798-98e0f341a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_metropolis(batch_size, Nc, action, num_therm, num_iters, step_size, save_freq=10):\n",
    "    \"\"\"Batched Metropolis sampler.\"\"\"\n",
    "    action_vals = []\n",
    "    accept_rates = []\n",
    "    \n",
    "    U = random_sun_element(batch_size, Nc=Nc)\n",
    "    ens = []\n",
    "    for i in tqdm.tqdm(range(-num_therm, num_iters)):\n",
    "        # Proposal\n",
    "        V = random_sun_element(batch_size, Nc=Nc, sigma=step_size)\n",
    "        Up = V @ U\n",
    "        dS = action(Up) - action(U)\n",
    "\n",
    "        # Accept / Reject\n",
    "        r = torch.rand((batch_size,))  # accept w/ prob = exp(-dS)\n",
    "        accept_mask = (r < torch.exp(-dS))[:, None, None]\n",
    "        U = torch.where(accept_mask, Up, U)\n",
    "\n",
    "        action_vals.append(grab(action(U).mean()))\n",
    "        accept_rates.append(grab(torch.sum(accept_mask) / batch_size))\n",
    "        if i >= 0 and (i+1) % save_freq == 0:\n",
    "            ens.append(U)\n",
    "    return torch.cat(ens), action_vals, accept_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223b0c6-14c7-4913-9d71-72565d689fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define physical theory\n",
    "beta = -1.0\n",
    "#action = SUNToyAction(beta)\n",
    "#action = SUNToyPolynomialAction(beta, [1.0, 0., 0.])\n",
    "action = SUNToyPolynomialAction(beta, [0.17, -0.65, 1.22])\n",
    "#action = SUNToyPolynomialAction(beta, [0.98, -0.63, -0.21])\n",
    "\n",
    "\n",
    "# Generate samples\n",
    "batch_size = 128\n",
    "num_therm = 1_000\n",
    "num_iters = 10_000\n",
    "save_freq = 10\n",
    "step_size = 0.9\n",
    "\n",
    "num_train = batch_size * (num_iters // save_freq)\n",
    "print(f'{num_train=}')\n",
    "\n",
    "U_train, action_vals, accept_rates = apply_metropolis(\n",
    "    batch_size = batch_size,\n",
    "    Nc = 2,\n",
    "    action = action,\n",
    "    num_therm = num_therm,\n",
    "    num_iters = num_iters,\n",
    "    step_size = step_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30358ff4-d9c5-4e3f-a7fb-eaa2ae243a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Metropolis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Metropolis Steps')\n",
    "\n",
    "axes[0].plot(*al.bin_data(action_vals, binsize=100))\n",
    "axes[0].set_ylabel('Average Action')\n",
    "\n",
    "axes[1].plot(*al.bin_data(accept_rates, binsize=100))\n",
    "axes[1].set_ylabel('Acceptance Rate')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1606b-c4d3-4468-8724-cd0a3c9c3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(grab(mat_angle(U_train[::10])[0][:, 0]), bins=50, density=True)\n",
    "ths = torch.linspace(-np.pi, np.pi, steps=101)\n",
    "dth = grab(ths[1]-ths[0])\n",
    "ths = torch.stack([ths, -ths], dim=-1)\n",
    "ps = grab((-action.value_eigs(ths)).exp()) * np.sin(grab(ths[:,0]))**2\n",
    "ps /= np.sum(ps, axis=-1, keepdims=True) * dth\n",
    "print(f'{ps.shape=}')\n",
    "ax.plot(grab(ths), ps, color='k', linestyle='--')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_ylabel('Density')\n",
    "#ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b21da-cc99-43dd-9d44-3ff718f0938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 3.0\n",
    "# diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "diffuser = PowerDiffusionSUN(sigma, alpha=1)\n",
    "\n",
    "times = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(4*len(times), 4), sharey=True)\n",
    "axes[0].set_ylabel('Density')\n",
    "#axes[0].set_yscale('log')\n",
    "\n",
    "bins = np.linspace(-np.pi, np.pi, num=51)\n",
    "\n",
    "U_0 = U_train.clone()[::10]\n",
    "for t, ax in zip(times, axes):\n",
    "    if t == 0:  # avoid sampling from HK at t=0\n",
    "        x_t, _, _ = mat_angle(U_0)\n",
    "    else:\n",
    "        U_t, _, _ = diffuser.diffuse(U_0, t*torch.ones(U_0.size(0)), n_iter=20)\n",
    "        x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=bins, density=True, color=cmap(t))\n",
    "    ax.set_xlabel(r'$\\theta_t$')\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "\n",
    "# also save histograms for future comparisons\n",
    "true_times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.10, 0.05, 0.01]\n",
    "true_hists = []\n",
    "for t in tqdm.tqdm(true_times):\n",
    "    hist_xt = np.zeros(len(bins)-1, dtype=np.float64)\n",
    "    for chunk_U in torch.chunk(U_0, 10):\n",
    "        U_t, _, _ = diffuser.diffuse(chunk_U, t*torch.ones(chunk_U.size(0)), n_iter=100)\n",
    "        x_t, _, _ = mat_angle(U_t)\n",
    "        hist_xt += np.histogram(grab(x_t[:, 0]), bins=bins)[0]\n",
    "    hist_xt /= U_0.size(0) * (bins[1]-bins[0])\n",
    "    true_hists.append((bins, hist_xt))\n",
    "\n",
    "# Plot uniform SU(2) Haar measure for comparison\n",
    "xs = torch.linspace(-np.pi, np.pi, 100)\n",
    "haar = (1 / np.pi) * torch.sin(xs)**2\n",
    "axes[-1].plot(grab(xs), grab(haar), ls='--', color='red', label='Haar Uniform')\n",
    "axes[-1].legend(frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc9f7c-2707-479f-bed3-39ee349f08f5",
   "metadata": {},
   "source": [
    "## Train a Score Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ff214-9f53-42d8-ac52-5c71797e053a",
   "metadata": {},
   "source": [
    "Now we must construct a score network that will take as input the eigenangle $\\theta$ and time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bb47e-3908-424a-b908-000451af94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SU2ScoreNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        assert input_dim % 2 == 1\n",
    "        self.nk = (input_dim - 1)//2\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # input_dim = 1 eigenangle + encoded time\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x_t, t):\n",
    "        assert len(x_t.shape) == 2, \\\n",
    "            'input eigenangles shape should be [batch_size, Nc-1]'\n",
    "        assert len(t.shape) == 1, \\\n",
    "            'times should only have a batch dimension'\n",
    "        cos_t = torch.cos(torch.exp(-10.0*(torch.arange(1,self.nk+1)/self.nk - 0.5))*t.unsqueeze(-1))\n",
    "        sin_t = torch.sin(torch.exp(-10.0*(torch.arange(1,self.nk+1)/self.nk - 0.5))*t.unsqueeze(-1))\n",
    "        inp = torch.cat([x_t, cos_t, sin_t], dim=-1)\n",
    "        return self.net(inp) * torch.sin(x_t) # enforce score -> 0 at endpoints\n",
    "\n",
    "\n",
    "def _test_su2_score_net():\n",
    "    batch_size = 10\n",
    "    Nc = 2\n",
    "    x = 2*np.pi*torch.rand((batch_size, 1)) - np.pi\n",
    "    t = torch.rand((batch_size,))\n",
    "    s_t = SU2ScoreNet()(x, t)\n",
    "\n",
    "    assert s_t.shape == x.shape, \\\n",
    "        '[FAILED: score net output must have same shape as input data]'\n",
    "    print('[PASSED]')\n",
    "\n",
    "_test_su2_score_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11c5d4-00fd-4d13-9cf2-e5484542d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_matching_loss_sun(U_0, diffuser, score_net, tol=1e-5):\n",
    "    batch_size = U_0.size(0)\n",
    "    t = tol + (1 - tol) * torch.rand((batch_size,))\n",
    "    # t = torch.zeros((batch_size,))\n",
    "    # t.exponential_(3) # exponential concentration for t\n",
    "    # t.clamp_(max=1.0)\n",
    "    sigma_t = diffuser.sigma_func(t)\n",
    "\n",
    "    U_t, xs, V = diffuser.diffuse(U_0, t, n_iter=50)\n",
    "    # x_0, _, _ = mat_angle(U_0)\n",
    "    x_t, P, Pinv = mat_angle(U_t)\n",
    "    x_t = x_t.to(dtype=t.dtype)\n",
    "\n",
    "    score = score_net(x_t[..., :-1], t)\n",
    "    # NOTE(gkanwar): Passing xs instead of (x_t - x_0) is important here\n",
    "    # NOTE(gkanwar): Scaling by sigma_t**2 results in a roughly scale-invariant true score,\n",
    "    # making training much more stable.\n",
    "    true_score_xs = sigma_t.unsqueeze(-1)**2 * sun_score_hk(xs[..., :-1], width=sigma_t)\n",
    "    true_score = extract_diag(Pinv @ V @ embed_diag(true_score_xs).to(V) @ adjoint(V) @ P).real[...,:-1]\n",
    "\n",
    "    diff = score - true_score\n",
    "    loss = torch.mean(diff**2)\n",
    "\n",
    "    # if torch.isnan(loss) or loss.item() > 100:\n",
    "    #     print('WARNING:', 'NaN' if torch.isnan(loss) else f'Loss blow-up! Loss = {loss.item()}')\n",
    "    #     big_inds = torch.nonzero(torch.abs(diff) > 100)\n",
    "    #     if big_inds.numel() > 0:\n",
    "    #         for idx in big_inds:\n",
    "    #             b, j = idx.tolist()\n",
    "    #             print(f'  sample {b}, angle {j}:')\n",
    "    #             print(f'    t = {t[b].item():.6f}')\n",
    "    #             print(f'    x_t - x_0 = {wrap((x_t - x_0)[b, j]).item():.6f}')\n",
    "    #             print(f'    score = {score[b, j].item():.6f}')\n",
    "    #             print(f'    true_score = {true_score[b, j].item():.6f}')\n",
    "    #             print(f'    diff = {diff[b, j].item():.6f}')\n",
    "    #             print(f'    K(x_t, t) = {sun_hk(x_t, width=sigma_t)[b]}')\n",
    "    #     else:\n",
    "    #         print('  (no large diffs found)')\n",
    "    #     print()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90fe27-8301-4a46-8b01-6d9871b52b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Make diffusion process\n",
    "# sigma = 1.1\n",
    "sigma = 3.0\n",
    "score_net = SU2ScoreNet(input_dim=51, hidden_dim=64)\n",
    "# diffuser = VarianceExpandingDiffusionSUN(sigma)\n",
    "diffuser = PowerDiffusionSUN(sigma, alpha=1)\n",
    "\n",
    "# Setup training hyperparams\n",
    "lr = 1e-4\n",
    "epochs = 100\n",
    "optimizer = torch.optim.Adam(params=score_net.parameters(), lr=lr)\n",
    "\n",
    "# Prepare dataloader\n",
    "batch_size = 1024\n",
    "# x_train, _, _ = mat_angle(U_train)\n",
    "dataset = TensorDataset(U_train) \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=get_device()))\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "score_net.train()\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (U_0_batch,) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # loss = score_matching_loss_eigs(x_0_batch, diffuser, score_net)\n",
    "        loss = score_matching_loss_sun(U_0_batch, diffuser, score_net)\n",
    "        # if loss.item() > 100:  # for now: skip the unstable outliers\n",
    "        #     continue\n",
    "        # if torch.isnan(loss):  # for now: skip NaNs\n",
    "        #     continue\n",
    "        torch.nn.utils.clip_grad_norm_(score_net.parameters(), max_norm=10.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += grab(loss)\n",
    "        losses.append(grab(loss))\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch}/{epochs} | Loss = {avg_loss:.6f}')\n",
    "    # losses.append(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb9d44-9413-4741-8eca-f93fcb4923cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss vs epochs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2.5))\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "#ax.set_yscale('log')\n",
    "xs, ys = al.bin_data(losses, binsize=100)\n",
    "xs = xs / (num_train/batch_size) # convert to epochs\n",
    "ax.plot(xs, ys, lw=0.75)\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba954ea-b28c-4923-a76f-818a3653a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check evaluated score vs noisy\n",
    "def _check_score():\n",
    "    U_0 = U_train.clone()\n",
    "    batch_size = U_0.size(0)\n",
    "\n",
    "    bins = np.linspace(-np.pi, np.pi, num=40)\n",
    "\n",
    "    ts = [0.05, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "    fig, axes = plt.subplots(2, 3, sharey=True)\n",
    "    for t_sc, ax in zip(tqdm.tqdm(ts), axes.flatten()):\n",
    "        # t = tol + (1 - tol) * torch.rand((batch_size,))\n",
    "\n",
    "        true_chunks = []\n",
    "        true_counts_chunks = []\n",
    "        for U_chunk in tqdm.tqdm(torch.chunk(U_0, 50), leave=False):\n",
    "            true = np.zeros(len(bins)-1, dtype=np.float64)\n",
    "            true_counts = np.zeros(len(bins)-1)\n",
    "            for _ in range(1):\n",
    "                t = t_sc * torch.ones((U_chunk.size(0),))\n",
    "                sigma_t = diffuser.sigma_func(t)\n",
    "                U_t, xs, V = diffuser.diffuse(U_chunk, t, n_iter=50)\n",
    "                # x_0, _, _ = mat_angle(U_0)\n",
    "                x_t, P, Pinv = mat_angle(U_t)\n",
    "                x_t = x_t.to(dtype=t.dtype)\n",
    "\n",
    "                true_score_xs = sigma_t.unsqueeze(-1)**2 * sun_score_hk(xs[..., :-1], width=sigma_t)\n",
    "                true_score = extract_diag(Pinv @ V @ embed_diag(true_score_xs).to(V) @ adjoint(V) @ P).real[...,:-1]\n",
    "                true += np.histogram(grab(x_t[...,0]), weights=grab(true_score[...,0]), bins=bins)[0]\n",
    "                true_counts += np.histogram(grab(x_t[...,0]), bins=bins)[0]\n",
    "            true_chunks.append(true)\n",
    "            true_counts_chunks.append(true_counts)\n",
    "\n",
    "        # NN score\n",
    "        xs_plot = np.linspace(-np.pi, np.pi, num=101)\n",
    "        score = score_net(torch.tensor(xs_plot).unsqueeze(-1), t_sc * torch.ones((len(xs_plot),)))\n",
    "        ax.plot(xs_plot, grab(score[...,0]))\n",
    "\n",
    "        # est of true score\n",
    "        true_chunks = np.stack(true_chunks)\n",
    "        true_counts_chunks = np.stack(true_counts_chunks)\n",
    "        est_true = al.bootstrap(\n",
    "            true_chunks, true_counts_chunks, Nboot=1000,\n",
    "            f=lambda x,c: np.sum(x, axis=0)/np.sum(c, axis=0))\n",
    "        xs = (bins[1:]+bins[:-1])/2\n",
    "        # ax.plot(xs, true/true_counts, color='k')\n",
    "        al.add_errorbar(est_true, ax=ax, xs=xs, color='k', linestyle='', marker='.')\n",
    "        # ax.plot(grab(x_t[..., 0]), grab(true_score[...,0]), marker='x', linestyle='')\n",
    "\n",
    "        ax.set_title(rf'$t = {t_sc:.02f}$')\n",
    "    fig.set_tight_layout(True)\n",
    "    fig.suptitle('Learned score vs data score estimates')\n",
    "    plt.show()\n",
    "_check_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e527f41-80d1-498f-8949-9f5c40098ed4",
   "metadata": {},
   "source": [
    "## Denoising Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b92cd-adab-4a8a-a98e-52827de40b77",
   "metadata": {},
   "source": [
    "Now we run the reverse process to generate new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f004c1-b6c1-49eb-b243-87a17602e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sun_gaussian(shape):\n",
    "    Nc, Nc_ = shape[-2:]\n",
    "    assert Nc == Nc_\n",
    "    return proj_to_algebra(torch.randn(shape) + 1j*torch.randn(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb221d4-8797-4bbd-a4d7-0e4f7f3f776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*torch.logsumexp(logw, dim=0) - torch.logsumexp(2*logw, dim=0)\n",
    "    return torch.exp(log_ess) / len(logw)\n",
    "def np_compute_ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*np.logaddexp.reduce(logw, axis=0) - np.logaddexp.reduce(2*logw, axis=0)\n",
    "    return np.exp(log_ess) / len(logw)\n",
    "\n",
    "def compute_kl_div(logp, logq):\n",
    "    # OV: reverse KL divergence since samples drawn from model q\n",
    "    return torch.mean(logq - logp)\n",
    "def np_compute_kl_div(logp, logq):\n",
    "    return np.mean(logq - logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad594ece-104e-4fe5-ad26-6046fd58610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_reverse_ODE_eigs(U_1, logr, score_net, diffuser, num_steps=200, verbose=False):\n",
    "    trajectories = {\n",
    "        'U_t': [],\n",
    "        'logp': [],\n",
    "        'logq': [],\n",
    "        'kl_div': [],\n",
    "        'ess': [],\n",
    "        't': [],\n",
    "        'Z': [],\n",
    "    }\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    batch_size = U_1.size(0)\n",
    "    x_1, V, V_inv = mat_angle(U_1)\n",
    "    x_t = x_1.clone()\n",
    "\n",
    "    logJ = 0.\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Get ODE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((batch_size,))) / sigma_t**2\n",
    "\n",
    "        # Skilling-Hutchinson Divergence Estimation\n",
    "        # func = lambda x: score_net(x, t*torch.ones((batch_size,))) / sigma_t**2\n",
    "        # div = estimate_divergence(func, x_t[..., :-1], num_estimates=10)\n",
    "        jac = torch.func.vmap(torch.func.jacfwd(lambda x: score_net(x[None], torch.tensor(t)[None])[0]))(x_t[..., :-1])\n",
    "        div = torch.einsum('...ii->...', jac) / sigma_t**2\n",
    "        \n",
    "        # Integration step in reverse time\n",
    "        score = torch.cat([score, -score.sum(-1, keepdim=True)], dim=-1)\n",
    "        x_t = x_t + 0.5 * g_t**2 * score * dt\n",
    "        # x_t = x_t + g_t**2 * score * dt\n",
    "        logJ = logJ + 0.5 * g_t**2 * div * dt\n",
    "\n",
    "        # Eigen-recomposition\n",
    "        D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "        U_t = V @ D @ V_inv\n",
    "        # FORNOW: SDE\n",
    "        # U_t = random_sun_matrix(U_t.size(0), Nc=Nc, sigma=g_t * dt**0.5) @ U_t\n",
    "        # x_t, V, Vinv = mat_angle(U_t)\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        logp = -action(U_t) + log_haar_su2(x_t)\n",
    "        logq = logr - logJ\n",
    "        Z = al.bootstrap(grab((logp - logq).exp()), Nboot=1000, f=al.rmean)\n",
    "        kl_div = al.bootstrap(grab(logp), grab(logq), Nboot=1000, f=np_compute_kl_div)\n",
    "        ess = al.bootstrap(grab(logp), grab(logq), Nboot=1000, f=np_compute_ess)\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps}')\n",
    "            print('logp =', logp.mean().item())\n",
    "            print('logq =', logq.mean().item())\n",
    "            print('Dkl =', kl_div)\n",
    "            print('ESS =', ess)\n",
    "            print()\n",
    "        trajectories['t'].append(t)\n",
    "        trajectories['U_t'].append(U_t)\n",
    "        trajectories['logp'].append(al.bootstrap(grab(logp), Nboot=1000, f=al.rmean))\n",
    "        trajectories['logq'].append(al.bootstrap(grab(logq), Nboot=1000, f=al.rmean))\n",
    "        trajectories['kl_div'].append(kl_div)\n",
    "        trajectories['ess'].append(ess)\n",
    "        trajectories['Z'].append(Z)\n",
    "\n",
    "    for key in ['Z', 'kl_div', 'ess', 'logp', 'logq']:\n",
    "        trajectories[key] = np.stack(trajectories[key], axis=1)\n",
    "    \n",
    "    return U_t, logJ, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3119b-6d7e-4e02-ab2c-98268d5992f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_haar_su2(x):\n",
    "    \"\"\"Computes log likelihood of SU(2) Haar uniform density.\"\"\"\n",
    "    x = x[:, 0]\n",
    "    log_sin2 = 2*torch.log(torch.abs(torch.sin(x)))\n",
    "    #log_norm = math.log(2 * np.pi**2)\n",
    "    log_norm = math.log(np.pi)\n",
    "    return log_sin2 - log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca990509-539a-411d-a064-988ef9e6613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data: 2x2 identity matrices\n",
    "batch_size = 4096\n",
    "Nc = 2\n",
    "# U_0 = U_train.clone()\n",
    "\n",
    "# Diffuse: U_0 -> U_1\n",
    "# U_1, _, _ = diffuser.diffuse(U_0, torch.ones((U_0.size(0),)), n_iter=25)\n",
    "# U_1 ~ Haar\n",
    "U_1 = random_un_haar_element(batch_size, Nc=Nc)\n",
    "U_1 *= (torch.linalg.det(U_1)**(-1/Nc) * torch.exp(2j*np.pi*torch.randint(Nc, size=(batch_size,))/Nc))[...,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d656e4-edce-4e72-954a-3e627bd65980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get prior log likelihood\n",
    "x_1, _, _ = mat_angle(U_1)\n",
    "logr = log_haar_su2(x_1)\n",
    "#logr = log_sun_hk(x_1, width=diffuser.sigma_func(1.0), eig_meas=False)\n",
    "print('avg logr =', logr.mean().item())\n",
    "print('std logr =', logr.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74634ca8-29d6-422d-b8b4-00a528e001b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U_0, logJ, history = solve_reverse_ODE_eigs(U_1, logr, score_net, diffuser, num_steps=200, verbose=True)\n",
    "x_0, _, _ = mat_angle(U_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecdb50-52d9-40aa-8f1d-7339145b3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reverse trajectories\n",
    "# times = [1.0, 0.75, 0.5, 0.25, 0.15, 0.01]\n",
    "times = true_times\n",
    "cmap = mpl.colormaps.get_cmap('viridis')\n",
    "fig, axes = plt.subplots(1, len(times), figsize=(2*len(times), 4), sharey=True)\n",
    "fig.suptitle('SU(2) Angular Spectral Density During Reverse Process')\n",
    "axes[0].set_ylabel('Density')\n",
    "xs = torch.linspace(-np.pi, np.pi, 100).unsqueeze(-1)\n",
    "for t, hist, ax in zip(times, true_hists, axes):\n",
    "    # Histogram denoised samples\n",
    "    step = int((1 - t) * len(history['U_t']))\n",
    "    U_t = history['U_t'][step]\n",
    "    x_t, _, _ = mat_angle(U_t)\n",
    "    ax.hist(grab(x_t[:, 0]), bins=50, density=True, color=cmap(t), alpha=0.65, label='Denoised samples')\n",
    "    # (hist[1][1:]+hist[1][:-1])/2, hist[0], width=hist[1][1]-hist[1][0]\n",
    "    ax.stairs(hist[1], hist[0], ec='red', color='none', label='Target Samples') # width=hist[1][1]-hist[1][0]\n",
    "    # ax.hist(grab(mat_angle(U_train)[0][:, 0]), bins=50, histtype='step', density=True, color='red', label='Target Samples')\n",
    "    # integrate score to get model distribution\n",
    "    sigma_t = max(0.1, diffuser.sigma_func(t))\n",
    "    bx = score_net(xs, t*torch.ones((xs.size(0),)))[...,0] / sigma_t**2\n",
    "    logp_hat = torch.cumsum(bx, dim=-1) * (xs[1,0]-xs[0,0])\n",
    "    p = grab((logp_hat + log_haar_su2(xs)).exp())\n",
    "    p /= np.sum(p)*(xs[1,0]-xs[0,0])\n",
    "    ax.plot(grab(xs), grab(p), color='k')\n",
    "    # Plot\n",
    "    ax.set_title(f'$t = {t}$')\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bd3ac-9b75-4601-8520-dcc29689d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _measure_Z():\n",
    "    th = torch.linspace(-np.pi, np.pi, steps=501)\n",
    "    x = torch.stack([th, -th], axis=-1)\n",
    "    U = embed_diag((1j*x).exp())\n",
    "    est_Z = (th[1]-th[0])*(-action(U) + log_haar_su2(x)).exp().sum()\n",
    "    return est_Z\n",
    "true_Z = _measure_Z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653af20-b4b2-42d5-8a79-e5eecbdea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(16, 4))\n",
    "\n",
    "axes[0].errorbar(history['t'], history['logp'][0], yerr=history['logp'][1])\n",
    "axes[0].set_ylabel(r'$\\log p$')\n",
    "\n",
    "axes[1].errorbar(history['t'], history['logq'][0], yerr=history['logq'][1])\n",
    "axes[1].set_ylabel(r'$\\log q$')\n",
    "\n",
    "axes[2].errorbar(history['t'], history['kl_div'][0], yerr=history['kl_div'][1])\n",
    "axes[2].set_ylabel(r'Reverse KL-divergence $D_{\\rm KL}(q || p)$')\n",
    "\n",
    "axes[3].errorbar(history['t'], history['ess'][0], yerr=history['ess'][1])\n",
    "axes[3].set_ylabel('ESS')\n",
    "\n",
    "axes[4].errorbar(history['t'], history['Z'][0], yerr=history['Z'][1])\n",
    "axes[4].axhline(true_Z, color='k', linestyle='--')\n",
    "axes[4].set_ylabel('Z')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f0f7d-5476-4f31-8616-991e0de4780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model likelihood\n",
    "logq = logr - logJ\n",
    "print('avg logq =', logq.mean().item())\n",
    "print('std logq =', logq.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1247d2-26f5-482d-ad04-5e04a6a6de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check normalization of q over time\n",
    "# NOTE(gkanwar): Done, see plot of Z which should remain constant\n",
    "# bootstrap over samples -> errors\n",
    "# NOTE(gkanwar): Done, see al.bootstrap calls\n",
    "# calculate divergence explicitly\n",
    "# NOTE(gkanwar): Done, see torch.func.jacfwd impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8c4c3-7f88-4dba-967b-4f566d6d72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target likelihood\n",
    "logp = -action(U_0) + log_haar_su2(x_0)\n",
    "print('avg logp =', logp.mean().item())\n",
    "print('std logp =', logp.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c549d6-2114-4da6-b8b1-6527860d51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Sample Size\n",
    "print('ESS = ', compute_ess(logp, logq).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacde69f-9565-4cd2-9ac9-75392e70ef12",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaff1ce-9977-4b18-8211-4f54dffb2ce2",
   "metadata": {},
   "source": [
    "**GK:** We will probably want some version of this when score_net accepts the matrices directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c981d1-da93-412a-a94c-4c4c0c558f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def solve_reverse_ODE(U_1, logr, score_net, diffuser, num_steps=200, verbose=False):\n",
    "    trajectories = {\n",
    "        'U_t': [],\n",
    "        'logp': [],\n",
    "        'logq': [],\n",
    "        'kl_div': [],\n",
    "        'ess': []\n",
    "    }\n",
    "    dt = 1 / num_steps\n",
    "    t = 1.0\n",
    "    U_t = U_1.clone()\n",
    "\n",
    "    logJ = 0.\n",
    "    for step in tqdm.tqdm(range(num_steps)):\n",
    "        # Eigendecompose\n",
    "        x_t, V, V_inv = mat_angle(U_t)\n",
    "\n",
    "        # Get ODE params\n",
    "        sigma_t = diffuser.sigma_func(t)\n",
    "        g_t = diffuser.noise_coeff(t)\n",
    "        score = score_net(x_t[..., :-1], t*torch.ones((U_t.size(0),))) / sigma_t**2\n",
    "\n",
    "        # Skilling-Hutchinson Divergence Estimation\n",
    "        func = lambda x: score_net(x, t*torch.ones((U_t.size(0),))) / sigma_t**2\n",
    "        div = estimate_divergence(func, x_t[..., :-1], num_estimates=10)\n",
    "        \n",
    "        # Integration step in reverse time\n",
    "        x_t = x_t + 0.5 * g_t**2 * score * dt\n",
    "        logJ = logJ + 0.5 * g_t**2 * div * dt\n",
    "\n",
    "        # Eigen-recomposition\n",
    "        D = embed_diag(torch.exp(1j * x_t)).to(V)\n",
    "        U_t = V @ D @ V_inv\n",
    "        t -= dt\n",
    "\n",
    "        # Collect and print metrics\n",
    "        logp = -action(U_t)\n",
    "        logq = logr - logJ\n",
    "        kl_div = compute_kl_div(logp, logq).item()\n",
    "        ess = compute_ess(logp, logq).item()\n",
    "        if verbose:\n",
    "            print(f'Step {step}/{num_steps}')\n",
    "            print('logp =', logp.mean().item())\n",
    "            print('logq =', logq.mean().item())\n",
    "            print('Dkl =', kl_div)\n",
    "            print('ESS =', ess)\n",
    "            print()\n",
    "        trajectories['U_t'].append(U_t)\n",
    "        trajectories['logp'].append(logp.mean().item())\n",
    "        trajectories['logq'].append(logq.mean().item())\n",
    "        trajectories['kl_div'].append(kl_div)\n",
    "        trajectories['ess'].append(ess)\n",
    " \n",
    "    return U_t, logJ, trajectories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
