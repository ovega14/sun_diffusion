{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede8bdbf-9126-4b6e-bed4-8c4675267f6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4042a-1c26-4010-a8b4-3fb04da94081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis as al\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "%matplotlib widget \n",
    "# requires ipympl package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399b9c7-ec6a-44dc-b61c-6be2fa897011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511cb6e-2bcc-4996-8b84-fff2e2b403d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(x):\n",
    "    return (x + np.pi) % (2*np.pi) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17dffb7-9c22-4088-b371-5bca4ae47dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefd63f-f95a-4c67-aa0c-42f374b93c1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SU(N) utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dbea1-39f7-4126-a12e-27f2cb6f010e",
   "metadata": {},
   "source": [
    "Almost entirely copied from StN project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547e24e-d182-4319-ac40-b3a0433527e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Matrix ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba8688-f301-4094-b3bd-ce014c25d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjoint(U):\n",
    "    return U.conj().swapaxes(-1,-2)\n",
    "def trace(U):\n",
    "    return U.diagonal(dim1=-2, dim2=-1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ce76c-16b4-4e80-8e9c-6d64f4ca3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(U, V):\n",
    "    \"\"\"Distance between two SU(Nc) matrices.\"\"\"\n",
    "    Nc = U.shape[-1]\n",
    "    return 1 - (1/Nc)*trace(U @ adjoint(V)).real\n",
    "\n",
    "def diag(U):\n",
    "    \"\"\"Just takes the diagonal format of a matrix U.\"\"\"\n",
    "    return torch.diagonal(U, dim1=-1, dim2=-2)\n",
    "\n",
    "def mat_angle(U):\n",
    "    \"\"\"\n",
    "    Diagonalizes an SU(Nc) matrix to get\n",
    "        \n",
    "        U = V exp(i th) Vinv\n",
    "    \n",
    "    and returns the eigen angles, as well as\n",
    "    eigenvecs through V, V^dagg\n",
    "    \"\"\"\n",
    "    L, V = torch.linalg.eig(U)\n",
    "    Vinv = torch.linalg.inv(V)\n",
    "    th = torch.angle(L)\n",
    "    return th, V, Vinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70becc-98c9-463c-aa00-871db7c7cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_trless(A):\n",
    "    \"\"\"Removes the trace of a matrix A.\"\"\"\n",
    "    Nc = A.shape[-1]\n",
    "    A -= (1/Nc)*torch.eye(Nc)*trace(A)[...,None,None]\n",
    "    return A\n",
    "\n",
    "def proj_sun_algebra(A):\n",
    "    \"\"\"\n",
    "    Projects a matrix A into the Lie algebra su(Nc) by\n",
    "    1.) Making it Hermitian by symmetrizing it,\n",
    "    2.) Making it traceless (see prev func).\n",
    "    \"\"\"\n",
    "    Nc = A.shape[-1]\n",
    "    Ap = (A + adjoint(A))/2\n",
    "    Ap = proj_trless(Ap)\n",
    "    return Ap\n",
    "\n",
    "def sample_sun_gaussian(shape):\n",
    "    \"\"\"\n",
    "    Samples a random, gaussian-distributed element of\n",
    "    the Lie algebra su(Nc).\n",
    "    \"\"\"\n",
    "    Nc, Nc_ = shape[-2:]\n",
    "    assert Nc == Nc_\n",
    "    return proj_sun_algebra(torch.randn(shape) + 1j*torch.randn(shape))\n",
    "\n",
    "def sample_su2_haar(n):\n",
    "    \"\"\"\n",
    "    Exactly samples a normal SU(2) group element from the Haar measure.\n",
    "    \"\"\"\n",
    "    z = np.random.normal(size=(n, 4))\n",
    "    x = z / np.linalg.norm(z, axis=-1, keepdims=True)\n",
    "    mat = [x[:,0]+1j*x[:,1], x[:,2]+1j*x[:,3], -x[:,2]+1j*x[:,3], x[:,0]-1j*x[:,1]]\n",
    "    return np.stack(mat, axis=-1).reshape(n, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd6eab-3086-448b-b8cf-ab4a7321a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERATORS\n",
    "### Normalization Tr[T^a T^b] = delta^{ab}\n",
    "_su2_gens = torch.stack([\n",
    "    torch.tensor([[0, 1], [1, 0]]),\n",
    "    torch.tensor([[0, -1j], [1j, 0]]),\n",
    "    torch.tensor([[1, 0], [0, -1]]),\n",
    "]) / np.sqrt(2)\n",
    "\n",
    "def test_su2_gens():\n",
    "    Delta = trace(_su2_gens[:,None] @ _su2_gens)\n",
    "    assert torch.allclose(Delta, torch.eye(len(_su2_gens)).cdouble())\n",
    "    print('Test SU(2) gens PASSED')\n",
    "\n",
    "if __name__ == '__main__': test_su2_gens()\n",
    "\n",
    "_su3_gens = torch.stack([\n",
    "    torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 0]]),\n",
    "    torch.tensor([[0, -1j, 0], [1j, 0, 0], [0, 0, 0]]),\n",
    "    torch.tensor([[1, 0, 0], [0, -1, 0], [0, 0, 0]]),\n",
    "    torch.tensor([[0, 0, 1], [0, 0, 0], [1, 0, 0]]),\n",
    "    torch.tensor([[0, 0, -1j], [0, 0, 0], [1j, 0, 0]]),\n",
    "    torch.tensor([[0, 0, 0], [0, 0, 1], [0, 1, 0]]),\n",
    "    torch.tensor([[0, 0, 0], [0, 0, -1j], [0, 1j, 0]]),\n",
    "    torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, -2]])/np.sqrt(3),\n",
    "]) / np.sqrt(2)\n",
    "\n",
    "def test_su3_gens():\n",
    "    \"\"\"\n",
    "    Checks whether the SU(3) generators satisfy\n",
    "\n",
    "        Tr(ta @ tb) = delta_ab\n",
    "    \"\"\"\n",
    "    Delta = trace(_su3_gens[:,None] @ _su3_gens)\n",
    "    assert torch.allclose(Delta, torch.eye(len(_su3_gens)).cdouble())\n",
    "    print('Test SU(3) gens PASSED')\n",
    "\n",
    "if __name__ == '__main__': test_su3_gens()\n",
    "\n",
    "def sun_gens(Nc):\n",
    "    if Nc == 2:\n",
    "        return _su2_gens\n",
    "    elif Nc == 3:\n",
    "        return _su3_gens\n",
    "    else:\n",
    "        raise ValueError(f'{Nc=}')\n",
    "\n",
    "def embed_sun_algebra(omega, Nc):\n",
    "    return torch.einsum('...x,xab->...ab', omega.cdouble(), sun_gens(Nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c519e4-789f-47a0-b335-018621511eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _su2_matrix_exp(A):\n",
    "    \"\"\"\n",
    "    Computes the (complex) matrix exponential map \n",
    "    of an su(2) Lie algebra matrix A. This is done by:\n",
    "\n",
    "    1.) Decomposing A = n \\dot Sigma into the SU(2) generators,\n",
    "    2.) checking that ||n|| ~ 0, i.e. how close A is to identity I,\n",
    "    3.) defining the norm as ||n||^2 + 0.1 where ||n||^2 is close to 0, otherwise just ||n||^2\n",
    "    4.) \n",
    "    \"\"\"\n",
    "    omega = torch.einsum('...ab,xba->...x', -1j*A, _su2_gens)\n",
    "    # assert torch.allclose(omega.imag, torch.tensor(0.0))\n",
    "    omega = omega.real\n",
    "    norm_sq = (omega**2).sum(dim=-1)\n",
    "    # need to be careful of sqrt branch cut at 0\n",
    "    ident_inds = torch.isclose(norm_sq, torch.tensor(0.0))\n",
    "    ident_inds_mat = torch.isclose(norm_sq[...,None,None], torch.tensor(0.0))\n",
    "    norm_omega = torch.where(ident_inds, norm_sq+0.1, norm_sq).sqrt()  # where true yield input, else other (condition, input, other)\n",
    "    omega_dot_sigma = embed_sun_algebra(omega, 2)\n",
    "    U = (\n",
    "        torch.cos(norm_omega/np.sqrt(2))[...,None,None]*torch.eye(2)\n",
    "        + 1j*torch.sinc(norm_omega/(np.pi*np.sqrt(2)))[...,None,None]*omega_dot_sigma\n",
    "    )\n",
    "    return torch.where(ident_inds_mat, torch.eye(2).cdouble()+A, U)\n",
    "\n",
    "def test_su2_matrix_exp():\n",
    "    omega = torch.randn((5,len(_su2_gens)))\n",
    "    omega = torch.cat([torch.zeros((1,len(_su2_gens))), omega])\n",
    "    def expiA(omega):\n",
    "        A = embed_sun_algebra(omega, Nc=2)\n",
    "        U = _su2_matrix_exp(1j*A)\n",
    "        return torch.stack([U.real, U.imag])\n",
    "    def expiA2(omega):\n",
    "        A = embed_sun_algebra(omega, Nc=2)\n",
    "        U = torch.matrix_exp(1j*A)\n",
    "        return torch.stack([U.real, U.imag])\n",
    "    assert torch.allclose(expiA(omega), expiA2(omega))\n",
    "    jac = torch.func.vmap(torch.func.jacfwd(expiA))(omega)\n",
    "    jac2 = torch.func.vmap(torch.func.jacfwd(expiA2))(omega)\n",
    "    assert torch.allclose(jac, jac2), f'{jac=} {jac2=}'\n",
    "\n",
    "if __name__ == '__main__': test_su2_matrix_exp()\n",
    "\n",
    "def sun_matrix_exp(A):\n",
    "    r\"\"\"exp(A) assuming $-iA \\in su(N)$\"\"\"\n",
    "    Nc = A.shape[-1]\n",
    "    if Nc == 2:\n",
    "        return _su2_matrix_exp(A)\n",
    "        # return torch.matrix_exp(A)\n",
    "    else:\n",
    "        return torch.matrix_exp(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4969d82-6efa-4f27-b943-e5a7bcee8845",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Autograd stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9466a-2648-48e4-b2e9-288ddd7cc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_sun(f):\n",
    "    \"\"\"\n",
    "    Wrapper for torch.func.grad that ensures derivs are evaluated along the\n",
    "    directions of Lie algebra generators.\n",
    "    \"\"\"\n",
    "    def grad(U):\n",
    "        Nc = U.shape[-1]\n",
    "        gens = sun_gens(Nc)\n",
    "        omega = torch.zeros(U.shape[:-2]+(len(gens),))\n",
    "        def ff(omega):\n",
    "            A = embed_sun_algebra(omega, Nc)\n",
    "            return f(U + 1j*A@U)\n",
    "        return torch.func.grad(ff)(omega)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfaa55-fcdc-4b43-af8f-728fe45aea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacfwd_sun(f):\n",
    "    def jacfwd(U):\n",
    "        Nc = U.shape[-1]\n",
    "        gens = sun_gens(Nc)\n",
    "        omega = torch.zeros(U.shape[:-2]+(len(gens),))\n",
    "        def ff(omega):\n",
    "            A = embed_sun_algebra(omega, Nc)\n",
    "            return f(U + 1j*A@U)\n",
    "        return torch.func.jacfwd(ff)(omega)\n",
    "    return jacfwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4efcb-b2af-4a0f-be6d-d1175d324cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hess_sun(f):\n",
    "    \"\"\"\n",
    "    Wrapper for torch.func.hessian that ensures derivs are evaluated along the\n",
    "    directions of Lie algebra generators.\n",
    "    \"\"\"\n",
    "    def hess(U):\n",
    "        Nc = U.shape[-1]\n",
    "        gens = sun_gens(Nc)\n",
    "        omega = torch.zeros(U.shape[:-2]+(len(gens),))\n",
    "        def ff(omega):\n",
    "            A = embed_sun_algebra(omega, Nc)\n",
    "            # TODO: make sure this shortcut works\n",
    "            # return f(U + 1j*A@U - 0.5*A@A@U)\n",
    "            return f(sun_matrix_exp(1j*A) @ U)\n",
    "        return torch.func.hessian(ff)(omega)\n",
    "    return hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64014913-1c34-4230-96a0-367a1d054e4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SU(3) canonical cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8939d-7845-4340-93c5-1b7801104a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_canonicalize_su3(thW):\n",
    "    # project onto hyperplane defined by sum_i theta_i = 0\n",
    "    thW[...,-1] -= torch.sum(thW, dim=-1)\n",
    "    v = thW.reshape(-1, 3)\n",
    "    \n",
    "    # wrap into canonical hexagon centered on I\n",
    "    # map from (a,b,c) into v = (th1, th2, th3)\n",
    "    U = 2*np.pi * torch.tensor([\n",
    "        [1, 0, -1],\n",
    "        [0, -1, 1],\n",
    "        [-1, 1, 0],\n",
    "    ])\n",
    "    # map from v into (a,b,c)\n",
    "    Uinv = torch.tensor([\n",
    "        [1, 0, -1],\n",
    "        [0, -1, 1],\n",
    "        [-1, 1, 0]\n",
    "    ]) / (6*np.pi)\n",
    "    kappa = Uinv @ torch.transpose(v, 0, 1) # ij,jb -> ib\n",
    "    a, b, c = kappa[0], kappa[1], kappa[2]\n",
    "    k = (b+c)/2\n",
    "    a -= k\n",
    "    b -= k\n",
    "    c -= k\n",
    "    a -= torch.round(a)\n",
    "    k = torch.round(b)\n",
    "    b -= k\n",
    "    c += k\n",
    "    b -= torch.round(b - (a+c)/2)\n",
    "    k = (b+c)/2\n",
    "    a -= k\n",
    "    b -= k\n",
    "    c -= k\n",
    "    a -= torch.round(a)\n",
    "    c -= torch.round(c - (a+b)/2)\n",
    "    return torch.transpose(U @ torch.stack([a,b,c], dim=0), 0, 1).reshape(thW.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to transform from (theta1, theta2, theta3) -> (alpha, beta) plane\n",
    "_su3_A = np.array([\n",
    "    [0, 1, -1] / np.sqrt(2),\n",
    "    [2, -1, -1] / np.sqrt(6),\n",
    "])\n",
    "_su3_Ainv = np.array([\n",
    "    [0, np.sqrt(2/3)],\n",
    "    [1 / np.sqrt(2), -1 / np.sqrt(6)],\n",
    "    [-1 / np.sqrt(2), -1 / np.sqrt(6)],\n",
    "])\n",
    "assert np.allclose(_su3_A @ _su3_Ainv, np.identity(2))\n",
    "\n",
    "\n",
    "# for nice plots in (alpha, beta) plane\n",
    "_su3_hex = (2*np.pi/3) * np.array([\n",
    "    (1,1,-2),\n",
    "    (2,-1,-1),\n",
    "    (1,-2,1),\n",
    "    (-1,-1,2),\n",
    "    (-2,1,1),\n",
    "    (-1,2,-1),\n",
    "    (1,1,-2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd37424-e6ca-4722-9d4a-2372be7f1fe2",
   "metadata": {},
   "source": [
    "# SU(N) heat-kernel distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813feed-e5f0-4456-914f-5ec7b87fc1f1",
   "metadata": {},
   "source": [
    "We use the standard definition of SU(N) Brownian motion as defined in stochastic quantization. For diffusion, we will want the forward process to be a variance-expanding scheme, so that for sufficiently large noise coefficient $\\sigma$, the final distribution is very nearly uniform.\n",
    "\n",
    "Unlike for elements of $\\mathbb{R}^n$, we **cannot \"fast-forward\"** $SU(N)$ Brownian motion by simply setting the scale of the sampled Gaussian noise to $\\sqrt{t}$. For example, two Brownian steps in the Euler-Heun scheme generate higher order terms, which play a role if $dt$ is not infinitesimal:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\exp(i \\sqrt{dt} A) \\exp(i \\sqrt{dt} B) &= \\exp(i \\sqrt{dt}(A+B) - \\frac{1}{2} dt [A, B]) + O(dt^{3/2}) \\\\\n",
    "&= \\exp(i \\sqrt{2 dt} C - \\frac{1}{2} dt [A, B]) + O(dt^{3/2}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here $C = (A+B)/\\sqrt{2} \\sim \\mathcal{N}(0, I)$ is corrcctly distributed, but we see a next-order non-trivial effect.\n",
    "\n",
    "However, the **heat-kernel** $K(U, t)$ has been derived in the case of $U(N)$ and $SU(N)$ groups (see e.g. Menotti and Onofri). It is a class function, structured as a periodically wrapped function of the eigenvalues. We will include the relevant Haar measure factor on the eigenvalues,\n",
    "$$\n",
    "h(\\theta_1, \\dots, \\theta_n) = \\prod_{i < j} |e^{i \\theta_i} - e^{i \\theta_j}|^2 = \\prod_{i < j} (2 - 2 \\cos(\\theta_i - \\theta_j)),\n",
    "$$\n",
    "and will find it convenient to work in terms of the \"unwrapped\" eigenvalue angles $x_i \\in \\mathbb{R}$ from which $\\lambda_i = e^{i x_i}$.\n",
    "Including the Haar measure, the $U(N)$ heat kernel comes out to\n",
    "$$\n",
    "K(x, t) = \\mathcal{N} \\prod_{i < j} \\left( \\frac{2 - 2 \\cos(x_i - x_j)}{|\\mathrm{sinc}(\\tfrac{1}{2}(x_i - x_j))|} \\right)\n",
    "\\exp(-\\frac{1}{2t} \\sum_i x_i^2) = \\mathcal{N} \\prod_{i < j} \\left( |x_i - x_j| \\sqrt{2 - 2 \\cos(x_i - x_j)} \\right)\n",
    "\\exp(-\\frac{1}{2t} \\sum_i x_i^2).\n",
    "$$\n",
    "Going to the $SU(N)$ heat kernel just requires an additional $\\delta(\\sum_i x_i)$.\n",
    "\n",
    "**TODO:** I don't understand the absolute value in the measure, need to work this out carefully at some point.\n",
    "\n",
    "**TODO:** The normalization is non-trivial, and I haven't managed to line this up with references. It isn't needed in the following, so we proceed without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d729cea-26cd-4e8e-a559-9b3ca2f21117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SU(N) heat kernel stuff\n",
    "\n",
    "# TODO(gkanwar): I think something is wrong with this measure term, given the\n",
    "# SU(2) results just below. Gotta derive this properly to check... e.g. it\n",
    "# might need to have the _periodic_ theta_i - theta_j, not x_i - x_j as the\n",
    "# argument?\n",
    "def _hk_meas_factor(delta):\n",
    "    return (2 - 2*np.cos(delta)) / np.abs(np.sinc(delta / (2*np.pi)))\n",
    "    # return np.sqrt(2 - 2*np.cos(delta)) * np.abs(delta)\n",
    "\n",
    "def _eval_hk_su2(theta, t, *, n_max=3):\n",
    "    # ignore ALL normalization factors for now\n",
    "    return sum(\n",
    "        _hk_meas_factor(2*theta + 4*np.pi*n) * np.exp(-(1/t) * (theta + 2*np.pi*n)**2) for n in range(-n_max, n_max)\n",
    "    )\n",
    "\n",
    "\n",
    "def _eval_hk_su3(theta1, theta2, t, *, n_max=3):\n",
    "    # ignore ALL normalization factors for now\n",
    "    theta3 = -theta1 - theta2  # sum to zero\n",
    "    \n",
    "    d12 = theta1 - theta2\n",
    "    d23 = theta2 - theta3\n",
    "    d31 = theta3 - theta1\n",
    "    \n",
    "    return sum(\n",
    "        _hk_meas_factor(d12+2*np.pi*(n1-n2)) *\n",
    "        _hk_meas_factor(d23+2*np.pi*(n2-(-n1-n2))) *\n",
    "        _hk_meas_factor(d31+2*np.pi*((-n1-n2)-n1)) *\n",
    "        np.exp(\n",
    "            -1/(2*t) * (theta1 + 2*np.pi*n1)**2\n",
    "            -1/(2*t) * (theta2 + 2*np.pi*n2)**2\n",
    "            -1/(2*t) * (theta3 + 2*np.pi*(-n1-n2))**2\n",
    "        )\n",
    "        for n1 in range(-n_max, n_max) for n2 in range(-n_max, n_max)\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_hk_sun_unwrapped(xs, t, meas_only=False):\n",
    "    assert len(t.shape) == 1 and t.shape[0] == xs.shape[0], 't should be batched'\n",
    "    xs = np.array(xs)\n",
    "    xn = -np.sum(xs, axis=-1, keepdims=True)\n",
    "    xs = np.concatenate([xs, xn], axis=-1)\n",
    "    \n",
    "    delta_x = np.stack([\n",
    "        xs[...,i] - xs[...,j]\n",
    "        for i in range(xs.shape[-1]) for j in range(i+1, xs.shape[-1])\n",
    "    ], axis=-1)  # all pairwise differences\n",
    "    \n",
    "    meas = np.prod(_hk_meas_factor(delta_x), axis=-1)\n",
    "    if meas_only:\n",
    "        return meas\n",
    "    weight = np.exp(-1/(2*t) * np.sum(xs**2, axis=-1))\n",
    "    return meas * weight\n",
    "\n",
    "\n",
    "def eval_hk_sun(thetas, t, *, n_max=3):\n",
    "    # promote t to batch, if needed\n",
    "    t = np.array(t)\n",
    "    if len(t.shape) == 0:\n",
    "        t = t * np.ones(thetas.shape[0])\n",
    "    assert t.shape == (thetas.shape[0],)\n",
    "    thetas = np.array(thetas)\n",
    "    total = 0\n",
    "    for ns in itertools.product(range(-n_max, n_max), repeat=thetas.shape[-1]):\n",
    "        ns = np.array(ns)\n",
    "        xs = thetas + 2*np.pi*ns\n",
    "        total = total + eval_hk_sun_unwrapped(xs, t)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0c4bd-9e58-49b0-8833-9bff1c9ab1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_hk_sun():\n",
    "    \"\"\"\n",
    "    Checks that the general `eval_hk_sun` reduces to the\n",
    "    hard-coded implementations for SU(2) and SU(3) when N = 2,3.\n",
    "    \"\"\"\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # SU(2)\n",
    "    theta = 6*np.pi*np.random.normal(size=(1024,))  # for multiple windings\n",
    "    va = _eval_hk_su2(theta, t=0.5)\n",
    "    vb = eval_hk_sun(theta[...,None], t=0.5)\n",
    "    assert np.allclose(va, vb), \\\n",
    "        '[FAILED SU(2): ]'\n",
    "    \n",
    "    # SU(3)\n",
    "    thetas = 6*np.pi*np.random.normal(size=(1024,2))  # Nc - 1 = 2\n",
    "    va = _eval_hk_su3(thetas[...,0], thetas[...,1], t=0.5)\n",
    "    vb = eval_hk_sun(thetas, t=0.5)\n",
    "    assert np.allclose(va, vb)\n",
    "    \n",
    "    print('[PASSED test_hk_sun]')\n",
    "\n",
    "if __name__ == '__main__': _test_hk_sun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46c2a1-0d35-46b9-8a42-d6b133d113c9",
   "metadata": {},
   "source": [
    "The measure factor over several $2\\pi$ intervals is shown below. It grows due to the behavior of $\\mathrm{sinc}(\\Delta)$ for large $\\Delta$, but is guaranteed to be cut off by the Gaussian weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b291ed9-ef6e-4f2f-a340-43aec653d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = np.linspace(-4*np.pi, 4*np.pi, 501)\n",
    "fig, ax = plt.subplots(1,1, figsize=(3.5, 2.5))\n",
    "ax.plot(deltas, _hk_meas_factor(deltas))\n",
    "ax.set_xlabel(r'$\\Delta$')\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f258b8-75c7-48e8-bf52-389a54a0237e",
   "metadata": {},
   "source": [
    "### SU(2)\n",
    "Heat kernel distribution over the phase $\\theta$ that sets the eigenvalue phases\n",
    "$$\\theta_1 = \\theta, \\qquad \\theta_2 = -\\theta.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b1899-29ea-43b9-9706-10b7848044cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_hk_su2():\n",
    "    \"\"\"\n",
    "    Simulates diffusion on SU(2). Collects histograms of eigenangles\n",
    "    at increasing times and compares thm to the analytic solution\n",
    "    from `_eval_hk_su2'. \n",
    "\n",
    "    Forward diffusion process is performed as \n",
    "        U_{t+1} = exp(iA*sqrt{dt}) @ U_t,\n",
    "    where A is a Lie algebra-valued noise matrix.\n",
    "    \"\"\"\n",
    "    Nc = 2\n",
    "    U0 = torch.stack([torch.eye(Nc)]*16000).cdouble()  # identity element of SU(2)\n",
    "    \n",
    "    dt = 0.01\n",
    "    ts = []\n",
    "    alpha = []\n",
    "    bins = np.linspace(0, np.pi, num=51)\n",
    "    \n",
    "    U1 = U0.clone()\n",
    "    for i in tqdm.tqdm(range(250)):\n",
    "        A = sample_sun_gaussian(U1.shape)\n",
    "        # assert torch.allclose(trace(A), torch.tensor(0.0).cdouble())\n",
    "        U1 = sun_matrix_exp(1j*A*np.sqrt(dt)) @ U1\n",
    "        if (i+1) % 50 == 0:\n",
    "            ts.append((i+1)*dt)\n",
    "            thetas, _, _ = mat_angle(U1)\n",
    "            alpha_U1 = np.abs(grab(thetas[...,0]))  # theta equivalent to -theta by symmetry\n",
    "            alpha.append(np.histogram(alpha_U1, bins=bins, density=True)[0])\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    xs = (bins[1:]+bins[:-1])/2\n",
    "    for i, (t, alpha_t) in enumerate(zip(ts, alpha)):\n",
    "        color = cmap(i / len(ts))\n",
    "        # data\n",
    "        ax.plot(xs, alpha_t, label=f't = {t:.2f}', color=color)\n",
    "        # anaytical\n",
    "        # TODO: the measure term of this result warps the answer so it is not\n",
    "        # properly symmetric and centered at pi/2...\n",
    "        ys = _eval_hk_su2(xs, t, n_max=10)\n",
    "        norm = np.sum(ys * (bins[1]-bins[0]))\n",
    "        print(f'{norm=}')\n",
    "        ys /= norm\n",
    "        ax.plot(xs, ys, color=color, linestyle='--')\n",
    "    ax.legend()\n",
    "    ax.set_title('SU(2) heat kernel (data vs analytic)')\n",
    "    ax.set_xticks([0, np.pi/2, np.pi])\n",
    "    ax.set_xticklabels(['0', r'$\\pi/2$', r'$\\pi$'])\n",
    "    ax.set_ylabel(r'Density')\n",
    "    ax.set_xlabel(r'Eig $\\theta$')\n",
    "_check_hk_su2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78218be7-16b4-43ed-adca-3ca33796ad74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SU(3)\n",
    "Heat kernel distribution over the space $(\\alpha, \\beta)$ that parameterize the constrained phases\n",
    "$$\n",
    "\\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\end{pmatrix} = A \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix},\n",
    "$$\n",
    "with $\\theta_3 = -\\theta_1 - \\theta_2$. Note that the Weyl group of permutations relates six triangular \"chambers\" that form the hexagonal space of all possible eigenvalue phases obtained from diagonalization. This total hexagonal is shown in the $(\\alpha, \\beta)$ plane for reference below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_hk_su3():\n",
    "    \"\"\"\n",
    "    Simulates (variance-expanding) diffusion on SU(3). \n",
    "    \n",
    "    Collects histograms of eigenangles at increasing times and \n",
    "    compares them to the analytic solution from `_eval_hk_su2'. \n",
    "\n",
    "    Forward diffusion process is performed as \n",
    "        U_{t+1} = exp(iA*sqrt{dt}) @ U_t,\n",
    "    where A is a Lie algebra-valued noise matrix.\n",
    "    \"\"\"\n",
    "    batch_size = 2**14\n",
    "    Nc = 3\n",
    "    U0 = torch.stack([torch.eye(Nc)] * batch_size).cdouble()\n",
    "    \n",
    "    dt = 0.01\n",
    "    ts = []\n",
    "    ab = []\n",
    "    \n",
    "    U1 = U0.clone()\n",
    "    for i in tqdm.tqdm(range(240)):\n",
    "        A = sample_sun_gaussian(U1.shape)\n",
    "        # assert torch.allclose(trace(A), torch.tensor(0.0).cdouble())\n",
    "        U1 = sun_matrix_exp(1j*A*np.sqrt(dt)) @ U1\n",
    "        if (i+1) % 40 == 0:\n",
    "            ts.append((i+1)*dt)\n",
    "            thetas, _, _ = mat_angle(U1)  # [batch_size, 3]\n",
    "            \n",
    "            # randomize order of thetas\n",
    "            thetas = grab(thetas)\n",
    "            for i in range(len(thetas)):\n",
    "                np.random.shuffle(thetas[i])\n",
    "            thetas = torch.tensor(thetas)\n",
    "            \n",
    "            canon_th = better_canonicalize_su3(thetas)\n",
    "            ab.append(np.einsum('ax,...x->a...', _su3_A, grab(canon_th)))  # project into Cartan subalgebra coordinate plane\n",
    "\n",
    "    bins = np.linspace(-5.5, 5.5, num=51)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(6, 4))\n",
    "    fig.suptitle('SU(3) heat kernel (data)')\n",
    "    fig2, axes2 = plt.subplots(2, 3, figsize=(6, 4))\n",
    "    fig2.suptitle('SU(3) heat kernel (analytic)')\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    xs = (bins[1:]+bins[:-1])/2\n",
    "    for i, (t, ab_t, ax, ax2) in enumerate(zip(ts, ab, axes.flatten(), axes2.flatten())):\n",
    "        # data\n",
    "        ax.hist2d(*ab_t, bins=bins)\n",
    "        \n",
    "        # analytical\n",
    "        a, b = np.meshgrid(bins, bins, indexing='ij')  # 2D grid over (alpha, beta) space\n",
    "        th = np.einsum('xa, a... -> x...', _su3_Ainv, np.stack((a,b), axis=0))  # map back into (th1, th2, th3) space\n",
    "        th = grab(better_canonicalize_su3(torch.tensor(th).moveaxis(0, -1)))  # fold into fundamental hexagon\n",
    "        v = eval_hk_sun(th[...,:2], t)\n",
    "        ax2.contourf(a, b, v)\n",
    "        \n",
    "        # outlines\n",
    "        ax.plot(*(_su3_A @ np.transpose(_su3_hex)), color='w', linestyle='--')\n",
    "        ax2.plot(*(_su3_A @ np.transpose(_su3_hex)), color='w', linestyle='--')\n",
    "        ax.set_aspect(1.0)\n",
    "        ax2.set_aspect(1.0)\n",
    "        ax.text(0.05, 0.95, f't={t:.2f}', fontsize=7, color='w', transform=ax.transAxes, ha='left', va='top')\n",
    "        ax2.text(0.05, 0.95, f't={t:.2f}', fontsize=7, color='w', transform=ax.transAxes, ha='left', va='top')\n",
    "    # ax.legend()\n",
    "    # ax.set_ylabel(r'Density')\n",
    "    # ax.set_xlabel(r'Eig $\\theta$')\n",
    "_check_hk_su3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d55b34-91bb-44da-ac0e-3e8c53b454c0",
   "metadata": {},
   "source": [
    "For comparison, the uniform Haar measure on the $(\\alpha, \\beta)$ plane of eigenvalues looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba77a07-90dd-49f6-8e07-ed560b061156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_haar():\n",
    "    \"\"\"\n",
    "    Plots the density of the Haar measure in the 2D plane\n",
    "    that parameterizes conjugacy classes of SU(3), i.e. \n",
    "    the space of eigenangles up to permutation.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1, figsize=(3.5, 3))\n",
    "    bins = np.linspace(-5.5, 5.5, num=51)\n",
    "    a, b = np.meshgrid(bins, bins, indexing='ij')\n",
    "    \n",
    "    th = np.einsum('xa,a...->x...', _su3_Ainv, np.stack((a,b), axis=0))\n",
    "    th = grab(better_canonicalize_su3(torch.tensor(th).moveaxis(0, -1)))\n",
    "    \n",
    "    # pairwise differences\n",
    "    v = (\n",
    "        (2 - 2*np.cos(th[...,0]-th[...,1])) * \n",
    "        (2 - 2*np.cos(th[...,1]-th[...,2])) * \n",
    "        (2 - 2*np.cos(th[...,2]-th[...,0]))\n",
    "    )\n",
    "\n",
    "    ax.contourf(a, b, v)\n",
    "    # outlines\n",
    "    ax.plot(*(_su3_A@np.transpose(_su3_hex)), color='w', linestyle='--')\n",
    "    ax.set_aspect(1.0)\n",
    "    ax.set_title('SU(3) Haar measure')\n",
    "_plot_haar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a2375-5285-4344-81be-3cd10401118f",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "We will want to sample from the given heat-kernel distribution at arbitrary times $t$. Since it is straightforward to wrap the distribution _after_ sampling, we work in the space of $x_i \\in \\mathbb{R}$. Since the measure seems to be relatively mild (at least for small $N_c$), we can simply apply a few hits of Metropolis resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8860201-8fe9-400e-881c-9b1a7a12ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hk_sun(batch_size, Nc, t, *, n_iter=3):\n",
    "    \"\"\"\n",
    "    Performs rejection sampling to draw samples from the\n",
    "    SU(N) heat kernel at time `t`.\n",
    "\n",
    "    Generates (unwrapped) isotropic proposals on the\n",
    "    eigenangles space, respecting SU(N) constraints.\n",
    "    Then iterates Metropolis for `n_iter` steps on \n",
    "    the angles.\n",
    "\n",
    "    Random eigenbasis is also generated by sampling\n",
    "    Haar-random unitary matrices.\n",
    "\n",
    "    Returns samples A in the algebra.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples to generate\n",
    "        Nc: Dimension of fundamental rep of SU(N)\n",
    "        t: Diffusion time (width of heat kernel), batched or scalar\n",
    "        n_iter: Number of Metropolis 'hits'\n",
    "    \"\"\"\n",
    "    # promote t to batch, if needed\n",
    "    t = np.array(t)\n",
    "    if len(t.shape) == 0:\n",
    "        t = t*np.ones((batch_size,))\n",
    "    assert t.shape == (batch_size,)\n",
    "\n",
    "    def propose():\n",
    "        \"\"\"Samples eigenangles from centered Gaussian on hyperplane.\"\"\"\n",
    "        xs = np.random.normal(size=(batch_size, Nc))\n",
    "        xs -= np.mean(xs, axis=-1, keepdims=True)  # sum to zero\n",
    "        xs *= np.sqrt(t)[...,None]  # match heat kernel width\n",
    "        return xs\n",
    "    \n",
    "    # sample eigenangles\n",
    "    xs = propose()\n",
    "    for i in range(n_iter):\n",
    "        xps = propose()\n",
    "        p = eval_hk_sun_unwrapped(xps[..., :-1], t, meas_only=True)\n",
    "        p /= eval_hk_sun_unwrapped(xs[..., :-1], t, meas_only=True)  # ratio b/w new, old points \n",
    "        u = np.random.random(size=p.shape)\n",
    "        xs[u < p] = xps[u < p]  # accept / reject step\n",
    "    \n",
    "    # OV: sample eigenvectors (see note below)\n",
    "    # xs_full = np.concatenate([xs, -np.sum(xs, axis=-1, keepdims=True)], axis=-1)  # full set of angles (sum to zero)\n",
    "    # A = np.empty((batch_size, Nc, Nc), dtype=np.complex128)\n",
    "    # for i in range(batch_size):  # sample Haar unitary\n",
    "    V, _ = np.linalg.qr(np.random.randn(batch_size, Nc, Nc) + 1j * np.random.randn(batch_size, Nc, Nc))\n",
    "    D = np.identity(xs.shape[-1]) * xs[...,None] # embed diagonal\n",
    "    A = V @ D @ adjoint(V)\n",
    "    \n",
    "    return xs, A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af3afe",
   "metadata": {},
   "source": [
    "**OV:** I am wondering if the above sampling of eigenvectors (and eigen-recomposition by conjugation thereafter) is valid. My initial question is: can we treat the eigenvectors as Haar-random at each slice in diffusion time $t$? It certainly wouldn't make sense for the eigenvectors $V$ to remain constant over diffusion time... but does random sampling of $V$ at each $t$ suffice?\n",
    "\n",
    "Recall that the heat kernel at time $t$ on ${\\rm SU}(N)$ satisfies $$\\partial_t K_t(U) = \\Delta K_t(U)$$ subject to initial condition $K_0(U) = \\delta(U)$, where $\\Delta$ is the Laplace-Beltrami operator on the ${\\rm SU}(N)$ manifold. \n",
    "\n",
    "We may always write an element $U \\in {\\rm SU}(N)$ as $U = V D V^\\dagger$, with $D = {\\rm diag}\\left(e^{i\\theta_1}, \\cdots, e^{i\\theta_N} \\right)$, and WLOG set $\\theta_N = -\\sum_{j=1}^{N-1}\\theta_j$. The Haar measure on ${\\rm SU}(N)$ should then factorize into eigenangles (in an $N-1$-dimensional space) and eigenvectors $V$ uniformly distributed over ${\\rm SU}(N) / \\mathbb{T}$, where $\\mathbb{T} \\leq {\\rm SU}(N)$ is the maximal torus in the group.\n",
    "\n",
    "Note: the heat kernel is *conjugation-invariant*-- one can easily show that $\\tilde{K}_t(U) := K_t(V U_t V^\\dagger)$ satisfies the same PDE as $K_t(U)$, so by a uniqueness argument, conjugation-invariance must follow. This implies that $\\forall V \\in {\\rm SU}(N)$, $$K_t(U) = K_t(V U V^\\dagger).$$ This means the heat kernel is only spectrum-dependent, so the distribution of the eigenvalues $D$ will evolve with $t$ on the Torus and the distribution of $V$ is Haar and independent of $t$. Simulating a diffusion trajectory will cause both eigenvalues AND eigenvectors to change, but simply sampling along the diffusion trajectory, i.e. sampling the marginal distribution of $U_t$, means that the eigenvectors are distributed according to a stationary Haar distribution. We are not sampling a full path, just a single point (marginal sample), so I think this is fine (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f295afd-9768-445a-bb49-b335a0127e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_sample_hk_su3():\n",
    "    batch_size = 16000\n",
    "    Nc = 3\n",
    "    ts = [0.4, 0.8, 1.2, 1.6, 2.0, 2.4]\n",
    "    \n",
    "    bins = np.linspace(-5.5, 5.5, num=51)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(6,4))\n",
    "    fig2, axes2 = plt.subplots(2, 3, figsize=(6,4))\n",
    "    fig.suptitle('SU(3) heat kernel (direct sampling)')\n",
    "    fig2.suptitle('SU(3) heat kernel (analytical)')\n",
    "    \n",
    "    for t, ax, ax2 in zip(ts, axes.flatten(), axes2.flatten()):\n",
    "        # samples\n",
    "        xs, _ = sample_hk_sun(batch_size, Nc=Nc, t=t, n_iter=30)\n",
    "        xs = grab(better_canonicalize_su3(torch.tensor(xs)))\n",
    "        ab = np.einsum('ax,...x->...a', _su3_A, xs)\n",
    "        ax.hist2d(ab[...,0], ab[...,1], bins=bins)\n",
    "        \n",
    "        # analytical\n",
    "        a, b = np.meshgrid(bins, bins, indexing='ij')\n",
    "        th = np.einsum('xa,a...->x...', _su3_Ainv, np.stack((a,b), axis=0))\n",
    "        th = grab(better_canonicalize_su3(torch.tensor(th).moveaxis(0, -1)))\n",
    "        v = eval_hk_sun(th[...,:2], t)\n",
    "        ax2.contourf(a, b, v)\n",
    "        \n",
    "        # outlines\n",
    "        ax.plot(*(_su3_A @ np.transpose(_su3_hex)), color='w', linestyle='--')\n",
    "        ax2.plot(*(_su3_A @ np.transpose(_su3_hex)), color='w', linestyle='--')\n",
    "        ax.text(0.05, 0.95, f't={t:.2f}', fontsize=7, color='w', transform=ax.transAxes, ha='left', va='top')\n",
    "        ax2.text(0.05, 0.95, f't={t:.2f}', fontsize=7, color='w', transform=ax.transAxes, ha='left', va='top')\n",
    "    plt.show()\n",
    "\n",
    "_test_sample_hk_su3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62d310-a586-4ecc-9ccf-3c70c56c41af",
   "metadata": {},
   "source": [
    "# SU(N) diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0f10b-eacd-4522-a41b-c4ca9d608822",
   "metadata": {},
   "source": [
    "We use the standard definition of SU(N) Brownian motion as defined in stochastic quantization to run the forward process. This will use the equivalent of the variance expanding scheme, so that for sufficiently large noise coefficient $\\sigma$, the final distribution is very nearly uniform. One should either use asymptotic times, or tricks of rescaling time evolution to make this exact -- let's not worry about these details for now.\n",
    "\n",
    "Conventionally, $t = 0$ will be the target distribution and $t = 1$ will be the uniform pure-noise distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6b194-a233-485f-b92a-6e8de3d4c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "### Need a matrix log func since PyTorch doesn't have this natively\n",
    "\n",
    "# scipy implementation - quite slow...\n",
    "def log_sun_scipy(V):\n",
    "    \"\"\"Compute matrix log for SU(N) matrices using SciPy.\"\"\"\n",
    "    V_np = grab(V)\n",
    "    A_np = np.array([scipy.linalg.logm(v) for v in V_np])\n",
    "    return torch.tensor(A_np, dtype=torch.cdouble)\n",
    "\n",
    "def log_sun_torch(V):  # warning: this torch implementation seems unstable\n",
    "    \"\"\"\n",
    "    Compute log(V) for V in SU(N) using eigendecomposition.\n",
    "    \n",
    "    Args: \n",
    "        V: Input batch of unitary matrices\n",
    "    Returns:\n",
    "        Matrix logarithm of `V`\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = torch.linalg.eig(V)\n",
    "    log_eigvals = torch.log(eigvals)  # principal branch\n",
    "    log_diag = torch.diag_embed(log_eigvals)\n",
    "    \n",
    "    Q = eigvecs\n",
    "    Qinv = torch.linalg.inv(Q)\n",
    "    A = Q @ log_diag @ Qinv\n",
    "    A = (A + adjoint(A)) / 2  # Hermitian\n",
    "    trA = trace(A)\n",
    "    A = A - trA[..., None, None] / A.shape[-1]  # traceless\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def sun_fwd_diffusion(U, t, *, sigma):\n",
    "    \"\"\"\n",
    "    Directly sample the forward diffusion process using the HK, \n",
    "    starting from initial data `U`.\n",
    "\n",
    "    Steps:\n",
    "        1.) Sample V from heat kenel at time s\n",
    "        2.) compute algebra perturbation A = log(V)\n",
    "        3.) apply perturbation to input U\n",
    "    \n",
    "    Args:\n",
    "        U: Initial SU(N) matrices at t=0\n",
    "        t: Scalar time in [0, 1]\n",
    "        sigma: Diffusion or noise scale at time `t`\n",
    "\n",
    "    Returns:\n",
    "        A: log matrix in algebra su(N)\n",
    "        Up: New SU(N) sample at time `t`\n",
    "    \"\"\"\n",
    "    batch_size, Nc, _ = U.shape\n",
    "    if callable(sigma):\n",
    "        s = sigma(t)\n",
    "    else:\n",
    "        s = sigma \n",
    "    _, A_np = sample_hk_sun(batch_size, Nc=Nc, t=s, n_iter=3)\n",
    "    A = torch.tensor(A_np, dtype=torch.cdouble)\n",
    "    #A = -1j * log_sun_scipy(V)\n",
    "    # A = -1j * log_sun_torch(V)\n",
    "    V = sun_matrix_exp(1j*A)\n",
    "    Up = V @ U\n",
    "    return A, Up\n",
    "\n",
    "\n",
    "def _test_sun_fwd_diffusion():\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size = 2 ** 4\n",
    "    Nc = 3\n",
    "    t = 0.5\n",
    "\n",
    "    sigma = lambda t: 5**t\n",
    "\n",
    "    U0 = torch.eye(Nc, dtype=torch.cdouble).repeat(batch_size, 1, 1)\n",
    "    A, Up = sun_fwd_diffusion(U0, t, sigma=sigma)\n",
    "\n",
    "    # Check A is Hermitian\n",
    "    assert torch.allclose(A, adjoint(A)), \\\n",
    "        '[FAILED: A is NOT Hermitian]'\n",
    "    \n",
    "    # Check A is traceless\n",
    "    trA = trace(A)\n",
    "    assert torch.allclose(trA, torch.zeros_like(trA)), \\\n",
    "        '[FAILED: A is NOT traceless]'\n",
    "    \n",
    "    # Check Up is unitary\n",
    "    I = torch.eye(Nc, dtype=torch.cdouble).repeat(batch_size, 1, 1)\n",
    "    assert torch.allclose(Up @ adjoint(Up), I), \\\n",
    "        '[FAILED: Up is NOT unitary]'\n",
    "    \n",
    "    # Check det(Up) = 1\n",
    "    dets = torch.linalg.det(Up)\n",
    "    assert torch.allclose(dets, torch.ones_like(dets)), \\\n",
    "        '[FAILED: det(Up) =/= 1]'\n",
    "    \n",
    "    print('[PASSED forward diffusion test]')\n",
    "\n",
    "\n",
    "if __name__ == '__main__': _test_sun_fwd_diffusion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72ed76-4ea6-459a-a20d-4662f254010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score(torch.nn.Module):\n",
    "    def __init__(self, Nc):\n",
    "        super().__init__()\n",
    "        n_gen = len(sun_gens(Nc))  # Nc^2 - 1\n",
    "        \n",
    "        # (U,t) -> A\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*Nc*Nc+1, 32),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(32, n_gen),\n",
    "            #torch.nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, U, t):\n",
    "        \"\"\"\n",
    "        Outputs a direction in the Lie algebra, i.e. \n",
    "        a tangent vector at the identity.\n",
    "\n",
    "        Args:\n",
    "            U: Input SU(N) matrix\n",
    "            t: Diffusion time\n",
    "\n",
    "        Returns:\n",
    "            Vector of Lie algebra generator coefficients\n",
    "        \"\"\"\n",
    "        U_re, U_im = U.real.flatten(1), U.imag.flatten(1)\n",
    "        assert len(t.shape) == 1, 't should be scalar batch'\n",
    "        Up = torch.cat([U_re, U_im, t.unsqueeze(1)], dim=1)\n",
    "        return self.net(Up)\n",
    "    \n",
    "\n",
    "def _test_score():\n",
    "    batch_size = 1\n",
    "    Nc = 3\n",
    "    shape = (batch_size, Nc, Nc)\n",
    "    A = sample_sun_gaussian(shape)\n",
    "    U = sun_matrix_exp(1j * A)\n",
    "    t = torch.randn((batch_size,))\n",
    "\n",
    "    score_net = Score(Nc)\n",
    "    out = score_net(U, t)\n",
    "    print('Input U shape:', U.shape)\n",
    "    print('Output coeffs shape:', out.shape)\n",
    "    \n",
    "    # Check that score maps to vector of algebra coeffs\n",
    "    assert out.shape == (batch_size, Nc**2 - 1), \\\n",
    "        '[FAILED: score network does not produce correct shape]'\n",
    "\n",
    "    # Recompose with the generators to produce algebra element\n",
    "    Ap = torch.einsum('bg, g... -> b...', out.cdouble(), _su3_gens)\n",
    "    Up = sun_matrix_exp(1j * Ap)\n",
    "    I = torch.eye(Nc).repeat(batch_size, 1, 1).cdouble()\n",
    "    assert torch.allclose(Up @ adjoint(Up), I), \\\n",
    "        '[FAILED: score net does not produce a unitary matrix]'\n",
    "    \n",
    "    print('[PASSED test simple score net]')\n",
    "\n",
    "if __name__ == '__main__': _test_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db735795",
   "metadata": {},
   "source": [
    "Let's try training a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE(gkanwar): This is identical to embed_sun_algebra() in utils above :)\n",
    "def combine_sun_basis(coeffs, basis):\n",
    "    \"\"\"\n",
    "    Creates an su(Nc) algebra element by forming the linear combination\n",
    "    of coefficients (`coeffs`) and generators (`basis`).\n",
    "\n",
    "    Number of coefficients should match number of generators, Nc^2 - 1.\n",
    "\n",
    "    Args:\n",
    "        coeffs: Vector of generator coefficients; [batch_size, Ng]\n",
    "        basis: Tensor of Lie group generators/basis vectors; [Ng, Nc, Nc]\n",
    "\n",
    "    Returns:\n",
    "        Algebra matrix; [batch_size, Nc, Nc]\n",
    "    \"\"\"\n",
    "    if coeffs.shape[-1] != len(basis):\n",
    "        raise ValueError('Number of coefficients must match number of generators')\n",
    "    return torch.einsum('bg, gij -> bij', coeffs.cdouble(), basis)\n",
    "\n",
    "\n",
    "def get_batch(batch_size, Nc, t, *, sigma):\n",
    "    \"\"\"Gets a batch of SU(N) matrices at time t.\"\"\"\n",
    "    _, A_np = sample_hk_sun(batch_size, Nc, sigma(t))\n",
    "    return torch.tensor(A_np, dtype=torch.cdouble)\n",
    "\n",
    "\n",
    "def target_score(A, t, *, sigma):\n",
    "    \"\"\"Estimate the true score using s(U) = -(1 / sigma^2) * log(U)\"\"\"\n",
    "    # TODO(gkanwar): factor of 2 here?\n",
    "    return A / (2*sigma(t)[...,None,None]**2)\n",
    "\n",
    "\n",
    "def score_matching_loss(A, U, score_net, t, *, sigma):\n",
    "    \"\"\"\n",
    "    Computes the score matching loss at the time / noise level\n",
    "    parameterized by sigma. \n",
    "    \"\"\"\n",
    "    pred_coeffs = score_net(U, t)\n",
    "    pred_score = combine_sun_basis(pred_coeffs, _su2_gens)\n",
    "    true_score = target_score(A, t, sigma=sigma)\n",
    "\n",
    "    fro_norm = torch.sum((pred_score - true_score).conj() * (pred_score - true_score), dim=(1, 2))\n",
    "    loss = torch.mean(fro_norm.real)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "Nc = 2\n",
    "batch_size = 2048\n",
    "epochs = 5000\n",
    "lr = 3e-4\n",
    "sigma = lambda t: 3.0*t**0.5\n",
    "\n",
    "# Init model and optimizer\n",
    "score_net = Score(Nc)\n",
    "optimizer = torch.optim.Adam(score_net.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    t = np.random.uniform(0.001, 1.0, size=batch_size)  # sample random t\n",
    "    # sigma_t = sigma(t)\n",
    "    A = get_batch(batch_size, Nc, t, sigma=sigma)\n",
    "    # in general U = V @ U1, in terms of data U1\n",
    "    # here we train to U1 = Id, i.e. sampled from p1 = delta(U1) at t=1\n",
    "    U = sun_matrix_exp(1j*A)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = score_matching_loss(A, U, score_net, torch.tensor(t), sigma=sigma)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist.append(grab(loss))\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"[Epoch: {epoch}/{epochs}] Loss = {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4d109-86f9-439d-ba3c-bd50c0d99664",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(loss_hist)\n",
    "ax.set_xlabel('train step')\n",
    "ax.set_ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_reverse(batch_size, score_net, steps=100):\n",
    "    dt = 1.0 / steps\n",
    "    tvals = torch.linspace(1.0, 0.0, steps+1)\n",
    "    # TODO: we should probably actually sample from Haar uniform here. The assumption\n",
    "    # is that we have diffused enough to forget the initial conditions... if we haven't\n",
    "    # then in any case the correct dist would be U_0 = V U_1 , V ~ K(V) in terms of initial\n",
    "    # data U_1 ~ p_1(U_1)\n",
    "    _, A_np = sample_hk_sun(batch_size, Nc, t=sigma(1.0)**2)  # sample from prior p_1\n",
    "    A = torch.tensor(A_np, dtype=torch.cdouble)\n",
    "    U = sun_matrix_exp(1j * A)\n",
    "\n",
    "    for t in tvals[:-1]:\n",
    "        sigma_t = sigma(t)\n",
    "        noise_scale = sigma_t * torch.tensor(2.0 * dt)**0.5\n",
    "\n",
    "        score_coeffs = score_net(U, t * torch.ones((batch_size,)))\n",
    "        score = combine_sun_basis(score_coeffs, _su2_gens)\n",
    "\n",
    "        noise_coeffs = torch.randn_like(score_coeffs)\n",
    "        noise = combine_sun_basis(noise_coeffs, _su2_gens)  # noise in Lie algebra\n",
    "\n",
    "        # Euler step in su(N)\n",
    "        delta = -dt * score + noise_scale * noise\n",
    "        #delta = -dt * target_score(U, t) + noise_scale * noise\n",
    "        U = sun_matrix_exp(1j * delta) @ U  # random walk on group mfld\n",
    "    \n",
    "    return U\n",
    "    \n",
    "\n",
    "U_new = sample_reverse(batch_size, score_net, steps=500)\n",
    "# Check that these are in the group\n",
    "assert torch.allclose(U_new @ adjoint(U_new), torch.eye(Nc).repeat(batch_size, 1, 1).cdouble()), \\\n",
    "    'U_new NOT unitary'\n",
    "assert torch.allclose(dets:=torch.linalg.det(U), torch.ones_like(dets)), \\\n",
    "    'det(U_new) =/= 1'\n",
    "print(f'Succesfully generated new SU({Nc}) matrices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_su2(U_sampled, t_eval=1.0):\n",
    "    \"\"\"\n",
    "    Plot histogram of SU(2) eigenangles and compare to analytical HK.\n",
    "    \"\"\"\n",
    "    # Get eigenangles\n",
    "    thetas, _, _ = mat_angle(U_sampled)\n",
    "    theta = thetas[:, 0]  # SU(2) has only one angle\n",
    "\n",
    "    # Histogram of sampled eigenangles\n",
    "    bins = np.linspace(0, np.pi, 100)\n",
    "    xs = (bins[:-1] + bins[1:]) / 2\n",
    "    hist, _ = np.histogram(np.abs(theta), bins=bins, density=True)\n",
    "\n",
    "    # Analytical HK (unnormalized)\n",
    "    ys = _eval_hk_su2(xs, t=sigma(t_eval)**2)\n",
    "    ys /= np.sum(ys * (bins[1] - bins[0]))  # normalize\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    ax.plot(xs, ys, label='Analytic HK', color='blue', linewidth=2)\n",
    "    ax.plot(xs, hist, label='Sampled', color='red', linewidth=2)\n",
    "    ax.set_title(r'SU(2) Eigenangle Distribution')\n",
    "    ax.set_xlabel(r'$|\\theta|$')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_xticks([0, np.pi/2, np.pi])\n",
    "    ax.set_xticklabels([r'$0$', r'$\\pi/2$', r'$\\pi$'])\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "validate_su2(U_new, t_eval=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00044f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "axes[0].set_ylabel('Density')\n",
    "thetas_new, _, _ = mat_angle(U_new)\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(thetas_new[:, i], bins=50, density=True, label='Diffusion')\n",
    "    ax.set_xlabel(rf'$\\theta_{i+1}$')\n",
    "fig.tight_layout()\n",
    "# should see mirror symmetry between the two histograms since th1 = -th2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = U_new.shape[0]\n",
    "Nc = 2\n",
    "t_eval_prior = 1.0\n",
    "t_eval_posterior = 0.1\n",
    "\n",
    "# Prior samples\n",
    "eigs_prior, _ = sample_hk_sun(batch_size, Nc, t=sigma(t_eval_prior)**2)\n",
    "eigs_prior = torch.tensor(eigs_prior, dtype=torch.float64)\n",
    "eigs_prior = torch.cat([eigs_prior, -torch.sum(eigs_prior, dim=-1, keepdim=True)], dim=-1)\n",
    "print('eigs prior shape:', eigs_prior.shape)\n",
    "U_prior = sun_matrix_exp(1j * combine_sun_basis(eigs_prior, _su2_gens)) \n",
    "\n",
    "theta_new, _, _ = mat_angle(U_new) \n",
    "theta_prior, _, _ = mat_angle(U_prior)\n",
    "theta_new = np.abs(theta_new[:, 0])\n",
    "theta_prior = np.abs(theta_prior[:, 0])\n",
    "\n",
    "\n",
    "bins = np.linspace(0, np.pi, 50)\n",
    "xs = (bins[1:] + bins[:-1]) / 2\n",
    "hist_new, _ = np.histogram(theta_new, bins=bins, density=True)\n",
    "hist_prior, _ = np.histogram(theta_prior, bins=bins, density=True)\n",
    "\n",
    "hk_post = _eval_hk_su2(xs, t=sigma(t_eval_posterior)**2)\n",
    "hk_prior = _eval_hk_su2(xs, t=sigma(t_eval_prior)**2)\n",
    "\n",
    "dx = bins[1] - bins[0]\n",
    "hk_post /= np.sum(hk_post * dx)\n",
    "hk_prior /= np.sum(hk_prior * dx)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.plot(xs, hist_new, label='Posterior (Diffusion)', color='blue', alpha=0.6)\n",
    "ax.plot(xs, hist_prior, label='Prior (Noisy)', color='orange', alpha=0.6)\n",
    "ax.plot(xs, hk_post, label='Posterior Analytic HK', color='darkblue', linewidth=2)\n",
    "ax.plot(xs, hk_prior, label='Prior Analytic HK', color='darkorange', linewidth=2)\n",
    "\n",
    "ax.set_xlabel(r'$|\\theta|$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(r'SU(2) Eigenangle Distributions')\n",
    "ax.set_xticks([0, np.pi/2, np.pi])\n",
    "ax.set_xticklabels([r'$0$', r'$\\pi/2$', r'$\\pi$'])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1104e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf3ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10c7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
